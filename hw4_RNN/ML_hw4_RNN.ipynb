{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_hw4_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r67y9UpchZ38",
        "colab_type": "text"
      },
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "本次作業是要讓同學接觸 NLP 當中一個簡單的 task —— 語句分類（文本分類）\n",
        "\n",
        "給定一個語句，判斷他有沒有惡意（負面標 0，正面標 1）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqYL_IvBj5CA",
        "colab_type": "text"
      },
      "source": [
        "## **goal**\n",
        "\n",
        "----- strong baseline -----\n",
        "0.82171   \n",
        "----- simple baseline -----\n",
        "0.76978"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLAwLYceDof7",
        "colab_type": "text"
      },
      "source": [
        "## **可以改的地方**\n",
        "###  1. def train_word2vec(x):  訓練 word to vector 的 word embedding   \n",
        "     model = word2vec.Word2Vec(x, size=250, window=5, min_count=5, workers=12, iter=10, sg=1) #原本的\n",
        "\n",
        "### 2. class LSTM_Net(nn.Module):  \n",
        "     def __init__(self, embedding, embedding_dim, hidden_dim, num_layers,  dropout=0.5, fix_embedding=True)\n",
        "\n",
        "### 3. training\n",
        "    def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device)\n",
        "### 4. Ensemble Learning   \n",
        "    Reference: http://violin-tao.blogspot.com/2018/01/ml-ensemble.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ajS_WskRo0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 設定路徑\n",
        "path_prefix = './'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YrAlczfM_w6",
        "colab_type": "text"
      },
      "source": [
        "### Download Dataset\n",
        "有三個檔案，分別是 training_label.txt、training_nolabel.txt、testing_data.txt\n",
        "\n",
        "- training_label.txt：有 label 的 training data（句子配上 0 or 1，+++$+++ 只是分隔符號，不要理它）\n",
        "    - e.g., 1 +++$+++ are wtf ... awww thanks !\n",
        "\n",
        "- training_nolabel.txt：沒有 label 的 training data（只有句子），用來做 semi-supervised learning\n",
        "    - ex: hates being this burnt !! ouch\n",
        "\n",
        "- testing_data.txt：你要判斷 testing data 裡面的句子是 0 or 1\n",
        "\n",
        "    >id,text\n",
        "\n",
        "    >0,my dog ate our dinner . no , seriously ... he ate it .\n",
        "\n",
        "    >1,omg last day sooon n of primary noooooo x im gona be swimming out of school wif the amount of tears am gona cry\n",
        "\n",
        "    >2,stupid boys .. they ' re so .. stupid !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2gwKORmuViJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gdown --id '1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8' --output data.zip\n",
        "!unzip data.zip\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hDIokoP6464",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is for filtering the warnings\n",
        "# 警告過濾器可以用來控制是否發出警告消息，警告過濾器是一些匹配規則和動作的序列。\n",
        "# 可以通過調用 filterwarnings() 將規則添加到過濾器，並通過調用 resetwarnings() 將其重置為默認狀態。\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc143hSvNGr6",
        "colab_type": "text"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtYubskPa-vZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import random\n",
        "\n",
        "# 固定隨機種子\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "    np.random.seed(seed)  # Numpy module.\n",
        "    random.seed(seed)  # Python random module.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "same_seeds(0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICDIhhgCY2-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utils.py\n",
        "# 這個 block 用來先定義一些等等常用到的函式\n",
        "\n",
        "def load_training_data(method, path='training_label.txt'):\n",
        "  if (method == 'unsupervised'):\n",
        "    # 把 training 時需要的 data 讀進來\n",
        "    # 如果是 'training_label.txt'，需要讀取 label，如果是 'training_nolabel.txt'，不需要讀取 label\n",
        "    if 'training_label' in path:\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            lines = [line.strip('\\n').split(' ') for line in lines]\n",
        "        x = [line[2:] for line in lines]\n",
        "        y = [line[0] for line in lines]\n",
        "        return x, y\n",
        "    else:\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            x = [line.strip('\\n').split(' ') for line in lines]\n",
        "        return x\n",
        "\n",
        "################################################################################\n",
        "\n",
        "  else:\n",
        "    # 把 training 時需要的 data 讀進來\n",
        "    # 如果是 'training_label.txt'，需要讀取 label，如果是 'training_nolabel.txt'，不需要讀取 label\n",
        "    if 'training_label' in path:\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            lines = [line.strip('\\n').split(' ') for line in lines]\n",
        "        x = [line[2:] for line in lines]\n",
        "        y = [line[0] for line in lines]\n",
        "        return x, y\n",
        "    else:\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            x = [line.strip('\\n').split(' ') for line in lines]\n",
        "            y = [outputs[i] for i in range(len(outputs))]\n",
        "        return x, y\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def load_testing_data(path='testing_data'):\n",
        "    # 把 testing 時需要的 data 讀進來\n",
        "    with open(path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]]\n",
        "        X = [sen.split(' ') for sen in X]\n",
        "    return X\n",
        "\n",
        "def evaluation(outputs, labels):\n",
        "    # outputs => probability (float)\n",
        "    # labels => labels\n",
        "    outputs[outputs>=0.5] = 1 # 大於等於 0.5 為無惡意 #原本的\n",
        "    outputs[outputs<0.5] = 0 # 小於 0.5 為有惡意 #原本的\n",
        "    correct = torch.sum(torch.eq(outputs, labels)).item()\n",
        "    return correct"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYE8UYQsNIxM",
        "colab_type": "text"
      },
      "source": [
        "### Train Word to Vector (supervised)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS4GdXlpymvi",
        "colab_type": "text"
      },
      "source": [
        "CBOW：From that context, predict the target word (Continuous Bag of Words or CBOW approach)\n",
        "\n",
        "Skip-Gram：From the target word, predict the context it came from (Skip-gram approach)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cgGWaF8_2S3q",
        "colab": {}
      },
      "source": [
        "# w2v.py\n",
        "# 這個 block 是用來訓練 word to vector 的 word embedding\n",
        "# 注意！這個 block 在訓練 word to vector 時是用 cpu，可能要花到 10 分鐘以上\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import argparse\n",
        "from gensim.models import word2vec\n",
        "\n",
        "# word2vec使用\n",
        "# size：特徵向量的維度，預設值為100\n",
        "# window：代表input word 與預測word的最大距離，若設為1，代表向左向右各看一個詞\n",
        "# min_count：該詞最少出現幾次，才可以被當作是訓練資料，例如min_count設為5，則出現5次以下的字詞都不會被放進來訓練\n",
        "# negative：大於0，代表採用negative sampling，設置多少個noise words\n",
        "#         例如「喜歡 麥當當 蘋果派 又 甜 又 香」，以「蘋果派」向左向右各看一個詞，就是「麥當當 蘋果派 又」\n",
        "# workers：多執行緒的數量\n",
        "# iter：迭代數量，預設為5\n",
        "# sg：演算法，預設為0，代表是CBOW，若設為1則是使用Skip-Gram\n",
        "\n",
        "def train_word2vec(x):\n",
        "    # 訓練 word to vector 的 word embedding\n",
        "    model = word2vec.Word2Vec(x, size=250, window=5, min_count=6, workers=12, iter=10, sg=1)\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 開始做字詞切割，有點像 1-gram\n",
        "    # training_label.txt 有 200,000 左右行，每行的詞數不等\n",
        "    # training_nolabel.txt 有 1,200,000 左右行，每行的詞數不等\n",
        "    print(\"loading training data ...\")\n",
        "    train_x, y = load_training_data('unsupervised', 'training_label.txt', )\n",
        "    train_x_no_label = load_training_data('unsupervised', 'training_nolabel.txt')\n",
        "    ############################################################################  \n",
        "    print(\"loading testing data ...\")\n",
        "    test_x = load_testing_data('testing_data.txt')\n",
        "    model = train_word2vec(train_x + test_x) #supervised\n",
        "    \n",
        "    print(\"saving model ...\")\n",
        "    model.save(os.path.join(path_prefix, 'w2v_all.model'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wHLtS0wNR6w",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfGKiOitk5ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocess.py\n",
        "# 這個 block 用來做 data 的預處理\n",
        "from torch import nn\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "class Preprocess():\n",
        "    def __init__(self, sentences, sen_len, w2v_path=\"./w2v.model\"):\n",
        "        self.w2v_path = w2v_path\n",
        "        self.sentences = sentences\n",
        "        self.sen_len = sen_len\n",
        "        self.idx2word = []\n",
        "        self.word2idx = {}\n",
        "        self.embedding_matrix = []\n",
        "    def get_w2v_model(self):\n",
        "        # 把之前訓練好的 word to vec 模型讀進來\n",
        "        self.embedding = Word2Vec.load(self.w2v_path)\n",
        "        self.embedding_dim = self.embedding.vector_size\n",
        "    def add_embedding(self, word):\n",
        "        # 把 word 加進 embedding，並賦予他一個隨機生成的 representation vector\n",
        "        # word 只會是 \"<PAD>\" 或 \"<UNK>\"\n",
        "          # PAD : 因為每個 batch 的單字長度要一致，所以我們要用 PAD 來填充過短的單字，主要用来进行字符补全。\n",
        "          # UNK : 如果輸入字元沒在字典裡出現過，就用 UNK 的索引替代它，用来替代一些未出现过的词或者低频词。\n",
        "        vector = torch.empty(1, self.embedding_dim)\n",
        "        torch.nn.init.uniform_(vector)\n",
        "        self.word2idx[word] = len(self.word2idx)\n",
        "        self.idx2word.append(word)\n",
        "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
        "    def make_embedding(self, load=True):\n",
        "        print(\"Get embedding ...\")\n",
        "        # 取得訓練好的 Word2vec word embedding\n",
        "        if load:\n",
        "            print(\"loading word to vec model ...\")\n",
        "            self.get_w2v_model()\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        # 製作一個 word2idx 的 dictionary\n",
        "        # 製作一個 idx2word 的 list\n",
        "        # 製作一個 word2vector 的 list\n",
        "        for i, word in enumerate(self.embedding.wv.vocab):\n",
        "            print('get words #{}'.format(i+1), end='\\r')\n",
        "            #e.g. self.word2index['he'] = 1 \n",
        "            #e.g. self.index2word[1] = 'he'\n",
        "            #e.g. self.vectors[1] = 'he' vector\n",
        "            self.word2idx[word] = len(self.word2idx)\n",
        "            self.idx2word.append(word)\n",
        "            self.embedding_matrix.append(self.embedding[word])\n",
        "        print('')\n",
        "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
        "        # 將 \"<PAD>\" 跟 \"<UNK>\" 加進 embedding 裡面\n",
        "        self.add_embedding(\"<PAD>\")\n",
        "        self.add_embedding(\"<UNK>\")\n",
        "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
        "        return self.embedding_matrix\n",
        "    def pad_sequence(self, sentence):\n",
        "        # 將每個句子變成一樣的長度\n",
        "        if len(sentence) > self.sen_len:\n",
        "            sentence = sentence[:self.sen_len]\n",
        "        else:\n",
        "            pad_len = self.sen_len - len(sentence)\n",
        "            for _ in range(pad_len):\n",
        "                sentence.append(self.word2idx[\"<PAD>\"])\n",
        "        assert len(sentence) == self.sen_len\n",
        "        return sentence\n",
        "    def sentence_word2idx(self):\n",
        "        # 把句子裡面的字轉成相對應的 index\n",
        "        sentence_list = []\n",
        "        for i, sen in enumerate(self.sentences):\n",
        "            print('sentence count #{}'.format(i+1), end='\\r')\n",
        "            sentence_idx = []\n",
        "            for word in sen:\n",
        "                if (word in self.word2idx.keys()):\n",
        "                    sentence_idx.append(self.word2idx[word])\n",
        "                else:\n",
        "                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
        "            # 將每個句子變成一樣的長度\n",
        "            sentence_idx = self.pad_sequence(sentence_idx)\n",
        "            sentence_list.append(sentence_idx)\n",
        "        return torch.LongTensor(sentence_list)\n",
        "    def labels_to_tensor(self, y):\n",
        "        # 把 labels 轉成 tensor\n",
        "        y = [int(label) for label in y]\n",
        "        return torch.LongTensor(y)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WJB7go5NWL0",
        "colab_type": "text"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XketwKs4lFfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data.py\n",
        "# 實作了 dataset 所需要的 '__init__', '__getitem__', '__len__'\n",
        "# 好讓 dataloader 能使用\n",
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "class TwitterDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    Expected data shape like:(data_num, data_len)\n",
        "    Data can be a list of numpy array or a list of lists\n",
        "    input data shape : (data_num, seq_len, feature_dim)\n",
        "    \n",
        "    __len__ will return the number of data\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.label = y\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is None: return self.data[idx]\n",
        "        return self.data[idx], self.label[idx]\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNJ8xWIMNa2r",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS6RJADulIq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.py\n",
        "# 這個 block 是要拿來訓練的模型\n",
        "import torch\n",
        "from torch import nn\n",
        "class LSTM_Net(nn.Module):\n",
        "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n",
        "        super(LSTM_Net, self).__init__()\n",
        "        # 製作 embedding layer\n",
        "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
        "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
        "        # 是否將 embedding fix 住，如果 fix_embedding 為 False，在訓練過程中，embedding 也會跟著被訓練\n",
        "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
        "        self.embedding_dim = embedding.size(1)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.classifier = nn.Sequential( nn.Dropout(dropout),\n",
        "                                         nn.Linear(hidden_dim, 1),\n",
        "                                         nn.Sigmoid() )\n",
        "    def forward(self, inputs):\n",
        "        inputs = self.embedding(inputs)\n",
        "        x, _ = self.lstm(inputs, None)\n",
        "        # x 的 dimension (batch, seq_len, hidden_size)\n",
        "        # 取用 LSTM 最後一層的 hidden state\n",
        "        x = x[:, -1, :] \n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWlpEL0sNc10",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QR4MMz-lR7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train.py\n",
        "# 這個 block 是用來訓練模型的\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "x_epoch = []\n",
        "y_train_acc = []\n",
        "y_val_acc = []\n",
        "def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n",
        "    model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數\n",
        "    criterion = nn.BCELoss() # 定義損失函數，這裡我們使用 binary cross entropy loss\n",
        "    t_batch = len(train) \n",
        "    v_batch = len(valid) \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr) # 將模型的參數給 optimizer，並給予適當的 learning rate\n",
        "    total_loss, total_acc, best_acc = 0, 0, 0\n",
        "    import matplotlib.pyplot as plt\n",
        "    for epoch in range(n_epoch):\n",
        "        x_epoch.append(epoch)\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss, total_acc = 0, 0\n",
        "        # 這段做 training\n",
        "        for i, (inputs, labels) in enumerate(train):\n",
        "            inputs = inputs.to(device, dtype=torch.long) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\n",
        "            labels = labels.to(device, dtype=torch.float) # device為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\n",
        "            optimizer.zero_grad() # 由於 loss.backward() 的 gradient 會累加，所以每次餵完一個 batch 後需要歸零\n",
        "            outputs = model(inputs) # 將 input 餵給模型\n",
        "            outputs = outputs.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\n",
        "            loss = criterion(outputs, labels) # 計算此時模型的 training loss\n",
        "            loss.backward() # 算 loss 的 gradient\n",
        "            optimizer.step() # 更新訓練模型的參數\n",
        "            correct = evaluation(outputs, labels) # 計算此時模型的 training accuracy\n",
        "            total_acc += (correct / batch_size)\n",
        "            total_loss += loss.item()\n",
        "            print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n",
        "            \tepoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\n",
        "        print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n",
        "        y_train_acc.append(total_acc/t_batch*100)\n",
        "\n",
        "        # 這段做 validation\n",
        "        model.eval() # 將 model 的模式設為 eval，這樣 model 的參數就會固定住\n",
        "        with torch.no_grad():\n",
        "            total_loss, total_acc = 0, 0\n",
        "            for i, (inputs, labels) in enumerate(valid):\n",
        "                inputs = inputs.to(device, dtype=torch.long) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\n",
        "                labels = labels.to(device, dtype=torch.float) # device 為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\n",
        "                outputs = model(inputs) # 將 input 餵給模型\n",
        "                outputs = outputs.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\n",
        "                loss = criterion(outputs, labels) # 計算此時模型的 validation loss\n",
        "                correct = evaluation(outputs, labels) # 計算此時模型的 validation accuracy\n",
        "                total_acc += (correct / batch_size)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
        "            if total_acc > best_acc:\n",
        "                # 如果 validation 的結果優於之前所有的結果，就把當下的模型存下來以備之後做預測時使用\n",
        "                best_acc = total_acc\n",
        "                torch.save(model, \"{}/ckpt.model\".format(model_dir))\n",
        "                print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))\n",
        "        y_val_acc.append(total_acc/v_batch*100)\n",
        "        print('epoch =', epoch, ', time cost = ', time.time()-epoch_start_time, ' sec(s)')\n",
        "        print('-----------------------------------------------')\n",
        "        model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數（因為剛剛轉成 eval 模式）\n",
        "    plt.plot(x_epoch, y_train_acc)\n",
        "    plt.plot(x_epoch, y_val_acc)\n",
        "    plt.title(\"acc_learning_curve\") # title\n",
        "    plt.ylabel(\"acc\") # y label\n",
        "    plt.xlabel(\"epoch\") # x label \n",
        "    plt.show()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfnKj0KXNeoz",
        "colab_type": "text"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EztIWqCmlZof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# main.py\n",
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from gensim.models import word2vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 通過 torch.cuda.is_available() 的回傳值進行判斷是否有使用 GPU 的環境，如果有的話 device 就設為 \"cuda\"，沒有的話就設為 \"cpu\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 處理好各個 data 的路徑\n",
        "train_with_label = os.path.join(path_prefix, 'training_label.txt')\n",
        "train_no_label = os.path.join(path_prefix, 'training_nolabel.txt')\n",
        "testing_data = os.path.join(path_prefix, 'testing_data.txt')\n",
        "w2v_path = os.path.join(path_prefix, 'w2v_all.model') # 處理 word to vec model 的路徑\n",
        "\n",
        "# 定義句子長度、要不要固定 embedding、batch 大小、要訓練幾個 epoch、learning rate 的值、model 的資料夾路徑\n",
        "sen_len = 40\n",
        "fix_embedding = True # fix embedding during training\n",
        "batch_size = 128\n",
        "epoch = 15\n",
        "lr = 0.001\n",
        "# model_dir = os.path.join(path_prefix, 'model/') # model directory for checkpoint model\n",
        "model_dir = path_prefix # model directory for checkpoint model\n",
        "\n",
        "print(\"loading data ...\") # 把 'training_label.txt' 跟 'training_nolabel.txt' 讀進來\n",
        "train_x, y = load_training_data('unsupervised', train_with_label)\n",
        "train_x_no_label = load_training_data('unsupervised', train_no_label)\n",
        "################################把 labeled 拿來用################################\n",
        "\n",
        "# 對 input 跟 labels 做預處理\n",
        "preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True)\n",
        "train_x = preprocess.sentence_word2idx()\n",
        "y = preprocess.labels_to_tensor(y)\n",
        "\n",
        "################################################################################\n",
        "\n",
        "# 製作一個 model 的對象\n",
        "model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=150, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
        "model = model.to(device) # device為 \"cuda\"，model 使用 GPU 來訓練（餵進去的 inputs 也需要是 cuda tensor）\n",
        "\n",
        "# 把 data 分為 training data 跟 validation data（將一部份 training data 拿去當作 validation data）\n",
        "# labeled training data 數量：20萬\n",
        "# unlabeled training data 數量：小於 120 萬，看自己設的閾值\n",
        "X_train, X_val, y_train, y_val = train_x[20000:], train_x[:20000], y[20000:], y[:20000]\n",
        "# X_train, X_val, y_train, y_val = train_x[100000:300000], train_x[:100000], y[100000:300000], y[:100000]\n",
        "\n",
        "# 把 data 做成 dataset 供 dataloader 取用\n",
        "train_dataset = TwitterDataset(X=X_train, y=y_train)\n",
        "val_dataset = TwitterDataset(X=X_val, y=y_val)\n",
        "\n",
        "# 把 data 轉成 batch of tensors\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = True,\n",
        "                                            num_workers = 8)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = False,\n",
        "                                            num_workers = 8)\n",
        "\n",
        "# 開始訓練\n",
        "training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iGnKmCiB4C5D"
      },
      "source": [
        "### Semi-suprivised with unlabeled data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOKWGyon4P74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# semi.py\n",
        "# 這個 block 用來對 train_x_no_label.txt 做預測\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "# good = 有信心資料，bad = 沒信心資料\n",
        "def semi_supervised(batch_size, train_x_no_label_loader, model, device, good, bad):\n",
        "    model.eval()\n",
        "    ret_output = []\n",
        "    good = 0\n",
        "    bad = 0\n",
        "    with torch.no_grad():\n",
        "        for i, inputs in enumerate(train_x_no_label_loader):\n",
        "            inputs = inputs.to(device, dtype=torch.long)\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs.squeeze()\n",
        "            # print(i, inputs, len(inputs), outputs, len(outputs))\n",
        "            for j in range(len(outputs)):\n",
        "              # print(len(outputs))\n",
        "              if(outputs[j]>=0.8):\n",
        "                outputs[j] = 1\n",
        "                good += 1\n",
        "              elif(outputs[j]<=0.2):\n",
        "                outputs[j] = 0\n",
        "                good += 1\n",
        "              else:\n",
        "                outputs[j] = 2000000\n",
        "                bad += 1\n",
        "            ret_output += outputs.int().tolist()        \n",
        "    return ret_output, good, bad"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3OGC_b551at",
        "colab_type": "text"
      },
      "source": [
        "### Predict train_x_no_label and Write to csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7zEgfnfG-sB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 開始測試模型並做預測\n",
        "print(\"loading training_nolabel data ...\")\n",
        "train_x_no_label = load_training_data('unsupervised', train_no_label)\n",
        "preprocess = Preprocess(train_x_no_label, sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True)\n",
        "train_x_no_label = preprocess.sentence_word2idx()\n",
        "train_x_no_label_dataset = TwitterDataset(X=train_x_no_label, y=None)\n",
        "train_x_no_label_loader = torch.utils.data.DataLoader(dataset = train_x_no_label_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = False,\n",
        "                                            num_workers = 8)\n",
        "print('\\nload model ...')\n",
        "model = torch.load(os.path.join(model_dir, 'ckpt.model'))\n",
        "good = 0\n",
        "bad = 0\n",
        "outputs, good, bad = semi_supervised(batch_size, train_x_no_label_loader, model, device, good, bad)\n",
        "print('num of good = ', good)\n",
        "print('num of bad = ', bad)\n",
        "\n",
        "# 寫到 csv 檔案供上傳 Kaggle\n",
        "tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(train_x_no_label))],\"label\":outputs})\n",
        "print(\"save csv ...\")\n",
        "tmp.to_csv(os.path.join(path_prefix, 'semi_supervised.csv'), index=False)\n",
        "print(\"Finish semi_supervised\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8OeBP0GU2Ee",
        "colab_type": "text"
      },
      "source": [
        "使用 label 好的 unlabeled data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YG10ISRUipA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_training_data(method, path='training_label.txt'):\n",
        "  if (method == 'unsupervised'):\n",
        "    # 把 training 時需要的 data 讀進來\n",
        "    # 如果是 'training_label.txt'，需要讀取 label，如果是 'training_nolabel.txt'，不需要讀取 label\n",
        "    if 'training_label' in path:\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            lines = [line.strip('\\n').split(' ') for line in lines]\n",
        "        x = [line[2:] for line in lines]\n",
        "        y = [line[0] for line in lines]\n",
        "        return x, y\n",
        "    else:\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            x = [line.strip('\\n').split(' ') for line in lines]\n",
        "        return x\n",
        "################################################################################\n",
        "  else:\n",
        "    # 把 training 時需要的 data 讀進來\n",
        "    # 如果是 'training_label.txt'，需要讀取 label，如果是 'training_nolabel.txt'，不需要讀取 label\n",
        "    if 'training_label' in path:\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            lines = [line.strip('\\n').split(' ') for line in lines]\n",
        "        x = [line[2:] for line in lines]\n",
        "        y = [line[0] for line in lines]\n",
        "        return x, y\n",
        "    else:\n",
        "        import pandas as pd\n",
        "        outputs = pd.read_csv(path_prefix + 'semi_supervised.csv')\n",
        "        outputs = outputs.values.tolist()\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            x = [line.strip('\\n').split(' ') for line in lines]\n",
        "            y = [outputs[i][1] for i in range(len(outputs))]\n",
        "        return x, y\n",
        "################################################################################\n",
        "\n",
        "def load_testing_data(path='testing_data'):\n",
        "    # 把 testing 時需要的 data 讀進來\n",
        "    with open(path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]]\n",
        "        X = [sen.split(' ') for sen in X]\n",
        "    return X\n",
        "\n",
        "def evaluation(outputs, labels):\n",
        "    # outputs => probability (float)\n",
        "    # labels => labels\n",
        "    outputs[outputs>=0.5] = 1 # 大於等於 0.5 為無惡意 #原本的\n",
        "    outputs[outputs<0.5] = 0 # 小於 0.5 為有惡意 #原本的\n",
        "    correct = torch.sum(torch.eq(outputs, labels)).item()\n",
        "    return correct"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egL1bDSwXhQC",
        "colab_type": "text"
      },
      "source": [
        "semi word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1CZCgdSXWVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# semi\n",
        "# w2v.py\n",
        "# 這個 block 是用來訓練 word to vector 的 word embedding\n",
        "# 注意！這個 block 在訓練 word to vector 時是用 cpu，可能要花到 10 分鐘以上\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import argparse\n",
        "from gensim.models import word2vec\n",
        "\n",
        "def train_word2vec(x):\n",
        "    # 訓練 word to vector 的 word embedding\n",
        "    # model = word2vec.Word2Vec(x, size=250, window=5, min_count=5, workers=12, iter=10, sg=1) #原本的\n",
        "    model = word2vec.Word2Vec(x, size=250, window=5, min_count=6, workers=12, iter=10, sg=1)\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 開始做字詞切割，有點像 1-gram\n",
        "    # training_label.txt 有 200,000 左右行，每行的詞數不等\n",
        "    # training_nolabel.txt 有 1,200,000 左右行，每行的詞數不等\n",
        "    print(\"loading training data ...\")\n",
        "    train_x, y = load_training_data('unsupervised', 'training_label.txt', )\n",
        "    ############################################################################  \n",
        "    #使用 semi #應該要用個函式包起來成為 pretty code\n",
        "    train_x_no_label, y_semi = load_training_data('semi_supervised', 'training_nolabel.txt')\n",
        "    # 把沒有信心的 data 不要做 label 資料\n",
        "    train_x_no_label_good = []\n",
        "    y_semi_good = []\n",
        "    for i in range (len(train_x_no_label)):\n",
        "      if(y_semi[i] == 1 or y_semi[i] == 0):\n",
        "        train_x_no_label_good.append(train_x_no_label[i])\n",
        "        y_semi_good.append(y_semi[i])\n",
        "    ############################################################################\n",
        "    print(\"loading testing data ...\")\n",
        "    test_x = load_testing_data('testing_data.txt')\n",
        "\n",
        "    # 使用 semi-supervised 過 strong\n",
        "    model = train_word2vec(train_x + train_x_no_label_good + test_x) #semi-supervised\n",
        "    \n",
        "    print(\"saving model ...\")\n",
        "    model.save(os.path.join(path_prefix, 'w2v_all.model'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVV8HdK2FkQt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e71b817-8859-4352-9e18-a4e20e5030cc"
      },
      "source": [
        "# semi\n",
        "# 通過 torch.cuda.is_available() 的回傳值進行判斷是否有使用 GPU 的環境，如果有的話 device 就設為 \"cuda\"，沒有的話就設為 \"cpu\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 處理好各個 data 的路徑\n",
        "train_with_label = os.path.join(path_prefix, 'training_label.txt')\n",
        "train_no_label = os.path.join(path_prefix, 'training_nolabel.txt')\n",
        "testing_data = os.path.join(path_prefix, 'testing_data.txt')\n",
        "w2v_path = os.path.join(path_prefix, 'w2v_all.model') # 處理 word to vec model 的路徑\n",
        "\n",
        "# 定義句子長度、要不要固定 embedding、batch 大小、要訓練幾個 epoch、learning rate 的值、model 的資料夾路徑\n",
        "sen_len = 40\n",
        "fix_embedding = True # fix embedding during training\n",
        "batch_size = 128\n",
        "epoch = 15\n",
        "lr = 0.001\n",
        "# model_dir = os.path.join(path_prefix, 'model/') # model directory for checkpoint model\n",
        "model_dir = path_prefix # model directory for checkpoint model\n",
        "\n",
        "print(\"loading data ...\") # 把 'training_label.txt' 跟 'training_nolabel.txt' 讀進來\n",
        "train_x, y = load_training_data('unsupervised', train_with_label)\n",
        "\n",
        "################################處理 semi data #################################\n",
        "\n",
        "train_x_no_label, y_semi = load_training_data('semi_supervised', train_no_label) #使用 semi\n",
        "# 把沒有信心的 data 不要做 label 資料\n",
        "train_x_no_label_good = []\n",
        "y_semi_good = []\n",
        "for i in range (len(train_x_no_label)):\n",
        "  if(y_semi[i] == 1 or y_semi[i] == 0):\n",
        "    train_x_no_label_good.append(train_x_no_label[i])\n",
        "    y_semi_good.append(y_semi[i])\n",
        "\n",
        "################################ 把unlabeled 拿來用 #############################\n",
        "\n",
        "preprocess = Preprocess(train_x + train_x_no_label_good, sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True)\n",
        "train_x = preprocess.sentence_word2idx()\n",
        "y = preprocess.labels_to_tensor(y + y_semi_good)\n",
        "\n",
        "################################################################################\n",
        "\n",
        "# 製作一個 model 的對象\n",
        "model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=150, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
        "model = model.to(device) # device為 \"cuda\"，model 使用 GPU 來訓練（餵進去的 inputs 也需要是 cuda tensor）\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_x[20000:500000], train_x[:20000], y[20000:500000], y[:20000]\n",
        "\n",
        "# 把 data 做成 dataset 供 dataloader 取用\n",
        "train_dataset = TwitterDataset(X=X_train, y=y_train)\n",
        "val_dataset = TwitterDataset(X=X_val, y=y_val)\n",
        "\n",
        "# 把 data 轉成 batch of tensors\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = True,\n",
        "                                            num_workers = 8)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = False,\n",
        "                                            num_workers = 8)\n",
        "\n",
        "# 開始訓練\n",
        "training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data ...\n",
            "Get embedding ...\n",
            "loading word to vec model ...\n",
            "get words #39674\n",
            "total words: 39676\n",
            "\n",
            "start training, parameter total:10160351, trainable:241351\n",
            "\n",
            "\n",
            "Train | Loss:0.28116 Acc: 87.620\n",
            "Valid | Loss:0.45686 Acc: 81.255 \n",
            "saving model with acc 81.255\n",
            "epoch = 0 , time cost =  26.407220125198364  sec(s)\n",
            "-----------------------------------------------\n",
            "\n",
            "Train | Loss:0.19551 Acc: 93.055\n",
            "Valid | Loss:0.43999 Acc: 81.852 \n",
            "saving model with acc 81.852\n",
            "epoch = 1 , time cost =  26.355143547058105  sec(s)\n",
            "-----------------------------------------------\n",
            "\n",
            "Train | Loss:0.18207 Acc: 93.543\n",
            "Valid | Loss:0.41907 Acc: 82.116 \n",
            "saving model with acc 82.116\n",
            "epoch = 2 , time cost =  26.625487327575684  sec(s)\n",
            "-----------------------------------------------\n",
            "\n",
            "Train | Loss:0.17139 Acc: 93.925\n",
            "Valid | Loss:0.44073 Acc: 81.907 \n",
            "epoch = 3 , time cost =  25.698182344436646  sec(s)\n",
            "-----------------------------------------------\n",
            "\n",
            "Train | Loss:0.16092 Acc: 94.290\n",
            "Valid | Loss:0.44976 Acc: 82.290 \n",
            "saving model with acc 82.290\n",
            "epoch = 4 , time cost =  26.27238178253174  sec(s)\n",
            "-----------------------------------------------\n",
            "\n",
            "Train | Loss:0.15079 Acc: 94.680\n",
            "Valid | Loss:0.42798 Acc: 82.275 \n",
            "epoch = 5 , time cost =  26.204041957855225  sec(s)\n",
            "-----------------------------------------------\n",
            "\n",
            "Train | Loss:0.13985 Acc: 95.099\n",
            "Valid | Loss:0.46966 Acc: 82.156 \n",
            "epoch = 6 , time cost =  26.245595693588257  sec(s)\n",
            "-----------------------------------------------\n",
            "\n",
            "Train | Loss:0.12881 Acc: 95.515\n",
            "Valid | Loss:0.50291 Acc: 82.076 \n",
            "epoch = 7 , time cost =  26.231582403182983  sec(s)\n",
            "-----------------------------------------------\n",
            "\n",
            "Train | Loss:0.11742 Acc: 95.955\n",
            "Valid | Loss:0.52360 Acc: 81.981 \n",
            "epoch = 8 , time cost =  26.753226041793823  sec(s)\n",
            "-----------------------------------------------\n",
            "\n",
            "Train | Loss:0.10744 Acc: 96.356\n",
            "Valid | Loss:0.52316 Acc: 81.991 \n",
            "epoch = 9 , time cost =  26.377891302108765  sec(s)\n",
            "-----------------------------------------------\n",
            "\n",
            "Train | Loss:0.09814 Acc: 96.753\n",
            "Valid | Loss:0.59794 Acc: 81.772 \n",
            "epoch = 10 , time cost =  26.388293266296387  sec(s)\n",
            "-----------------------------------------------\n",
            "\n",
            "Train | Loss:0.09070 Acc: 97.020\n",
            "Valid | Loss:0.58936 Acc: 81.698 \n",
            "epoch = 11 , time cost =  26.004661798477173  sec(s)\n",
            "-----------------------------------------------\n",
            "\n",
            "Train | Loss:0.08383 Acc: 97.286\n",
            "Valid | Loss:0.63209 Acc: 81.623 \n",
            "epoch = 12 , time cost =  26.19000506401062  sec(s)\n",
            "-----------------------------------------------\n",
            "\n",
            "Train | Loss:0.07826 Acc: 97.503\n",
            "Valid | Loss:0.65971 Acc: 81.444 \n",
            "epoch = 13 , time cost =  26.245283603668213  sec(s)\n",
            "-----------------------------------------------\n",
            "\n",
            "Train | Loss:0.07375 Acc: 97.676\n",
            "Valid | Loss:0.66088 Acc: 81.618 \n",
            "epoch = 14 , time cost =  26.42542839050293  sec(s)\n",
            "-----------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF5YQrupNfCS",
        "colab_type": "text"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X2wkdAYxHYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test.py\n",
        "# 這個 block 用來對 testing_data.txt 做預測\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def testing(batch_size, test_loader, model, device):\n",
        "    model.eval()\n",
        "    ret_output = []\n",
        "    with torch.no_grad():\n",
        "        for i, inputs in enumerate(test_loader):\n",
        "            inputs = inputs.to(device, dtype=torch.long)\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs.squeeze()\n",
        "            outputs[outputs>=0.5] = 1 # 大於等於 0.5 為負面\n",
        "            outputs[outputs<0.5] = 0 # 小於 0.5 為正面\n",
        "            ret_output += outputs.int().tolist()\n",
        "               \n",
        "    return ret_output"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fQeaQNeNm3L",
        "colab_type": "text"
      },
      "source": [
        "### Predict test_data and Write to csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFvjFQopxVrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 開始測試模型並做預測\n",
        "print(\"loading testing data ...\")\n",
        "\n",
        "test_x = load_testing_data(testing_data) #變成 list list\n",
        "preprocess = Preprocess(test_x, sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True)\n",
        "test_x = preprocess.sentence_word2idx()\n",
        "test_dataset = TwitterDataset(X=test_x, y=None)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = False,\n",
        "                                            num_workers = 8)\n",
        "print('\\nload model ...')\n",
        "model = torch.load(os.path.join(model_dir, 'ckpt.model'))\n",
        "outputs = testing(batch_size, test_loader, model, device)\n",
        "\n",
        "# 寫到 csv 檔案供上傳 Kaggle\n",
        "tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"label\":outputs})\n",
        "print(\"save csv ...\")\n",
        "tmp.to_csv(os.path.join(path_prefix, 'predict.csv'), index=False)\n",
        "print(\"Finish Predicting\")\n",
        "\n",
        "# 以下是使用 command line 上傳到 Kaggle 的方式\n",
        "# 需要先 pip install kaggle、Create API Token，詳細請看 https://github.com/Kaggle/kaggle-api 以及 https://www.kaggle.com/code1110/how-to-submit-from-google-colab\n",
        "# kaggle competitions submit [competition-name] -f [csv file path]] -m [message]\n",
        "# e.g., kaggle competitions submit ml-2020spring-hw4 -f output/predict.csv -m \"......\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}