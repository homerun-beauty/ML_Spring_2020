{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_hw2_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSL2CMEzmQvB",
        "colab_type": "text"
      },
      "source": [
        "#**Homework 2 - Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgBlZ0ptW-fE",
        "colab_type": "text"
      },
      "source": [
        "## **goal**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox7joE3aZkh-",
        "colab_type": "text"
      },
      "source": [
        "Binary classification is one of the most fundamental problem in machine learning. In this tutorial, you are going to build linear binary classifiers to predict whether the income of an indivisual exceeds 50,000 or not. We presented a discriminative and a generative approaches, the logistic regression(LR) and the linear discriminant anaysis(LDA). You are encouraged to compare the differences between the two, or explore more methodologies. Although you can finish this tutorial by simpliy copying and pasting the codes, we strongly recommend you to understand the mathematical formulation first to get more insight into the two algorithms. Please find [here](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Logistic%20Regression%20(v3).pdf) and [here](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Classification%20(v3).pdf) for more detailed information about the two algorithms.\n",
        "\n",
        "二元分類是機器學習中最基礎的問題之一，在這份教學中，你將學會如何實作一個線性二元分類器，來根據人們的個人資料，判斷其年收入是否高於 50,000 美元。我們將以兩種方法: logistic regression 與 generative model，來達成以上目的，你可以嘗試了解、分析兩者的設計理念及差別。針對這兩個演算法的理論基礎，可以參考李宏毅老師的教學投影片 [logistic regression](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Logistic%20Regression%20(v3).pdf) 與 [generative model](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Classification%20(v3).pdf)。\n",
        "\n",
        "若有任何問題，歡迎來信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9at05dSXIG1",
        "colab_type": "text"
      },
      "source": [
        " ----- strong baseline -----   0.89102\n",
        "\n",
        "----- simple baseline -----   0.88675"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkNW5cQmohoo",
        "colab_type": "text"
      },
      "source": [
        "#Dataset\n",
        "\n",
        "This dataset is obtained by removing unnecessary attributes and balancing the ratio between positively and negatively labeled data in the [**Census-Income (KDD) Data Set**](https://archive.ics.uci.edu/ml/datasets/Census-Income+(KDD)), which can be found in [**UCI Machine Learning Repository**](https://archive.ics.uci.edu/ml/index.php). Only preprocessed and one-hot encoded data (i.e. *X_train*,  *Y_train* and *X_test*) will be used in this tutorial. Raw data (i.e. *train.csv* and *test.csv*) are provided to you in case you are interested in it.\n",
        "\n",
        "這個資料集是由 [**UCI Machine Learning Repository**](https://archive.ics.uci.edu/ml/index.php) 的 [**Census-Income (KDD) Data Set**](https://archive.ics.uci.edu/ml/datasets/Census-Income+(KDD)) 經過一些處理而得來。為了方便訓練，我們移除了一些不必要的資訊，並且稍微平衡了正負兩種標記的比例。事實上在訓練過程中，只有 X_train、Y_train 和 X_test 這三個經過處理的檔案會被使用到，train.csv 和 test.csv 這兩個原始資料檔則可以提供你一些額外的資訊。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww4-VJoJqE-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gdown --id '1KSFIRh0-_Vr7SdiSCZP1ItV7bXPxMD92' --output data.tar.gz # download the dataset as data.tar.gz\n",
        "!tar -zxvf data.tar.gz # 解壓\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRXI0kf0W4Bd",
        "colab_type": "text"
      },
      "source": [
        "#Logistic Regression\n",
        "\n",
        "In this section we will introduce logistic regression first. We only present how to implement it here, while mathematical formulation and analysis will be omitted. You can find more theoretical detail in [Prof. Lee's lecture](https://www.youtube.com/watch?v=hSXFuypLukA).\n",
        "\n",
        "首先我們會實作 logistic regression，針對理論細節說明請參考[李宏毅老師的教學影片](https://www.youtube.com/watch?v=hSXFuypLukA)\n",
        "\n",
        "###Preparing Data\n",
        "\n",
        "Load and normalize data, and then split training data into training set and development set.\n",
        "\n",
        "下載資料，並且對每個屬性做正規化，處理過後再將其切分為訓練集與發展集。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NzAmkzU2MAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "X_train_fpath = './data/X_train' # set the file path\n",
        "Y_train_fpath = './data/Y_train'\n",
        "X_test_fpath = './data/X_test'\n",
        "output_fpath = './output_{}.csv'\n",
        "\n",
        "# Parse csv files to numpy array\n",
        "with open(X_train_fpath) as f: # open the file\n",
        "    next(f) # next() 返回迭代器的下一個項目，平時會與 iter() 合用。\n",
        "    X_train = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\n",
        "with open(Y_train_fpath) as f:\n",
        "    next(f)\n",
        "    Y_train = np.array([line.strip('\\n').split(',')[1] for line in f], dtype = float)\n",
        "with open(X_test_fpath) as f:\n",
        "    next(f)\n",
        "    X_test = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\n",
        "\n",
        "def _normalize(X, train = True, specified_column = None, X_mean = None, X_std = None):\n",
        "    # This function normalizes specific columns of X.\n",
        "    # The mean and standard variance of training data will be reused when processing testing data.\n",
        "    #\n",
        "    # Arguments:\n",
        "    #     X: data to be processed\n",
        "    #     train: 'True' when processing training data, 'False' for testing data\n",
        "    #     specific_column: indexes of the columns that will be normalized. If 'None', all columns\n",
        "    #         will be normalized.\n",
        "    #     X_mean: mean value of training data, used when train = 'False'\n",
        "    #     X_std: standard deviation of training data, used when train = 'False'\n",
        "    # Outputs:\n",
        "    #     X: normalized data\n",
        "    #     X_mean: computed mean value of training data\n",
        "    #     X_std: computed standard deviation of training data\n",
        "\n",
        "    if specified_column == None:\n",
        "        specified_column = np.arange(X.shape[1]) # 產生一個 column 為 X.shape[1] 長度的array\n",
        "    if train:\n",
        "        X_mean = np.mean(X[:, specified_column] ,0).reshape(1, -1) \n",
        "        # reshape 特殊用法：mat (or array).reshape(c, -1)\n",
        "        # 必須是矩陣或數組格式，才能使用 .reshape(c, -1) 函数 \n",
        "        # 表示重組此矩陣或是數組，以 c 行 d 列的形式表示（-1的作用就在此，自動計算 d：d = 數組或矩陣的所有元素個數 /c, d 必須為整數，不然報錯）\n",
        "        # reshape(-1, e) 即列數固定，行數重新計算\n",
        "\n",
        "        # mean : axis = 0：壓縮行，對每一列求平均值，返回 1* n 矩陣\n",
        "        X_std  = np.std(X[:, specified_column], 0).reshape(1, -1)\n",
        "\n",
        "    X[:,specified_column] = (X[:, specified_column] - X_mean) / (X_std + 1e-8) # normalize\n",
        "     \n",
        "    return X, X_mean, X_std\n",
        "\n",
        "def _train_dev_split(X, Y, dev_ratio = 0.25):\n",
        "    # This function spilts data into training set and development set.\n",
        "    train_size = int(len(X) * (1 - dev_ratio))\n",
        "    return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]\n",
        "\n",
        "# Normalize training and testing data\n",
        "X_train, X_mean, X_std = _normalize(X_train, train = True) # 決定是否標準化\n",
        "X_test, _, _= _normalize(X_test, train = False, specified_column = None, X_mean = X_mean, X_std = X_std)\n",
        "    \n",
        "# Split data into training set and development set\n",
        "# training set：訓練集是用來訓練模型，占了數據的絕大部分。\n",
        "# development set：用來對模型測試的資料集，通過測試來不斷優化模型。\n",
        "# test set：訓練結束後，對訓練出的模型進行最後一次評估的數據集。\n",
        "dev_ratio = 0.1 # 可以自行調配 train, dev 數量\n",
        "X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio = dev_ratio)\n",
        "\n",
        "train_size = X_train.shape[0]\n",
        "dev_size = X_dev.shape[0]\n",
        "test_size = X_test.shape[0]\n",
        "data_dim = X_train.shape[1]\n",
        "print('Size of training set: {}'.format(train_size))\n",
        "print('Size of development set: {}'.format(dev_size))\n",
        "print('Size of testing set: {}'.format(test_size))\n",
        "print('Dimension of data: {}'.format(data_dim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imgCeBDoApdb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "###Some Useful Functions\n",
        "\n",
        "Some functions that will be repeatedly used when iteratively updating the parameters.\n",
        "\n",
        "這幾個函數可能會在訓練迴圈中被重複使用到。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSDAw5LTAs2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _shuffle(X, Y):\n",
        "    # This function shuffles two equal-length list/array, X and Y, together.\n",
        "    randomize = np.arange(len(X)) # 產生元素從 0 開始至 len(x)-1 的 1*len(x) array (公差 = 1)\n",
        "    np.random.shuffle(randomize) # np.random.shuffle(): Modify a sequence in-place by shuffling its contents\n",
        "    return (X[randomize], Y[randomize])\n",
        "\n",
        "def _sigmoid(z):\n",
        "    # Sigmoid function can be used to calculate probability.\n",
        "    # To avoid overflow, minimum/maximum output value is set.\n",
        "    return np.clip(1 / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))\n",
        "\n",
        "def _f(X, w, b):\n",
        "    # This is the logistic regression function, parameterized by w and b\n",
        "    #\n",
        "    # Arguements:\n",
        "    #     X: input data, shape = [batch_size, data_dimension]\n",
        "    #     w: weight vector, shape = [data_dimension, ]\n",
        "    #     b: bias, scalar\n",
        "    # Output:\n",
        "    #     predicted probability of each row of X being positively labeled, shape = [batch_size, ]\n",
        "    return _sigmoid(np.matmul(X, w) + b) # np.matmul(): 矩陣相乘\n",
        "\n",
        "def _predict(X, w, b):\n",
        "    # This function returns a truth value prediction for each row of X \n",
        "    # by rounding the result of logistic regression function.\n",
        "    return np.round(_f(X, w, b)).astype(np.int) # np.round: 對傳入的數據進行四捨五入，如果ngigits不傳，默認是0（就是說保留整數部分）\n",
        "    \n",
        "def _accuracy(Y_pred, Y_label):\n",
        "    # This function calculates prediction accuracy\n",
        "    acc = 1 - np.mean(np.abs(Y_pred - Y_label))\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxJdfhEEOYwg",
        "colab_type": "text"
      },
      "source": [
        "### Functions about gradient and loss\n",
        "\n",
        "Please refers to [Prof. Lee's lecture slides](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Logistic%20Regression%20(v3).pdf)(p.12) for the formula of gradient and loss computation.\n",
        "\n",
        "請參考[李宏毅老師上課投影片](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Logistic%20Regression%20(v3).pdf)第 12 頁的梯度及損失函數計算公式。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqYkUgLjOWi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _cross_entropy_loss(y_pred, Y_label):\n",
        "    # This function computes the cross entropy.\n",
        "    #\n",
        "    # Arguements:\n",
        "    #     y_pred: probabilistic predictions, float vector\n",
        "    #     Y_label: ground truth labels, bool vector\n",
        "    # Output:\n",
        "    #     cross entropy, scalar\n",
        "    cross_entropy = -np.dot(Y_label, np.log(y_pred)) - np.dot((1 - Y_label), np.log(1 - y_pred)) # np.log 下沒寫默認底數是自然對數 \n",
        "    return cross_entropy\n",
        "\n",
        "def _gradient(X, Y_label, w, b):\n",
        "    # This function computes the gradient of cross entropy loss with respect to weight w and bias b.\n",
        "    y_pred = _f(X, w, b)\n",
        "    pred_error = Y_label - y_pred\n",
        "    w_grad = -np.sum(pred_error * X.T, 1)\n",
        "    b_grad = -np.sum(pred_error)\n",
        "    return w_grad, b_grad\n",
        "\n",
        "def _gradient_L1(X, Y_label, w, b, lambda_L1):\n",
        "    # This function computes the gradient of cross entropy loss with respect to weight w and bias b.\n",
        "    y_pred = _f(X, w, b)\n",
        "    pred_error = Y_label - y_pred\n",
        "    w_grad = -np.sum(pred_error * X.T, 1) + lambda_L1*(w/(abs(w)))\n",
        "    b_grad = -np.sum(pred_error)\n",
        "    return w_grad, b_grad\n",
        "\n",
        "def _gradient_L2(X, Y_label, w, b, lambda_L2):\n",
        "    # This function computes the gradient of cross entropy loss with respect to weight w and bias b.\n",
        "    y_pred = _f(X, w, b)\n",
        "    pred_error = Y_label - y_pred\n",
        "    w_grad = -np.sum(pred_error * X.T, 1) + 2*lambda_L2*w\n",
        "    b_grad = -np.sum(pred_error)\n",
        "    return w_grad, b_grad"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXEFuqydaA34",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "\n",
        "Everything is prepared, let's start training! \n",
        "\n",
        "Mini-batch gradient descent is used here, in which training data are split into several mini-batches and each batch is fed into the model sequentially for losses and gradients computation. Weights and bias are updated on a mini-batch basis.\n",
        "\n",
        "Once we have gone through the whole training set,  the data have to be re-shuffled and mini-batch gradient desent has to be run on it again. We repeat such process until max number of iterations is reached.\n",
        "\n",
        "我們使用小批次梯度下降法來訓練。訓練資料被分為許多小批次，針對每一個小批次，我們分別計算其梯度以及損失，並根據該批次來更新模型的參數。當一次迴圈完成，也就是整個訓練集的所有小批次都被使用過一次以後，我們將所有訓練資料打散並且重新分成新的小批次，進行下一個迴圈，直到事先設定的迴圈數量達成為止。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6yNUeG9aBR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Zero initialization for weights ans bias\n",
        "# w = np.zeros((data_dim,)) # 產生 data_dim*1 的 zero array\n",
        "# b = np.zeros((1,)) # 產生 1*1 的 zero array\n",
        "\n",
        "# 這裡使用 one array，因為當 L1_regularization 時，一開始 w 無法在 0 的位置產生梯度，因此初始化我們採用 one array\n",
        "w = np.ones((data_dim,)) # 產生 data_dim*1 的 one array\n",
        "b = np.ones((1,)) # 產生 1*1 的 one array\n",
        "\n",
        "# Some parameters for training # 可調看看不同參數   \n",
        "max_iter = 550\n",
        "# batchsize：中文翻譯為批大小（批尺寸）。簡單點說，批量大小將決定我們一次訓練的樣本數目。batch_size將影響到模型的優化程度和速度。\n",
        "# 為什麼需要有 Batch_Size： batchsize 的正確選擇是為了在記憶體效率和記憶體容量之間尋找最佳平衡。\n",
        "# 相對於正常資料集，如果 Batch_Size 過小，訓練資料就會非常難收斂，從而導致 underfitting。\n",
        "# 增大 Batch_Size, 相對處理速度加快。\n",
        "# 增大 Batch_Size, 所需記憶體容量增加（epoch的次數需要增加以達到最好的結果）\n",
        "# 這裡我們發現上面兩個矛盾的問題，因為當 epoch 增加以後同樣也會導致耗時增加從而速度下降。因此我們需要尋找最好的 Batch_Size。\n",
        "# 再次重申：Batch_Size 的正確選擇是為了在記憶體效率和記憶體容量之間尋找最佳平衡。\n",
        "batch_size = 16\n",
        "learning_rate = 0.08\n",
        "\n",
        "# Keep the loss and accuracy at every iteration for plotting\n",
        "train_loss = []\n",
        "dev_loss = []\n",
        "train_acc = []\n",
        "dev_acc = []\n",
        "lambda_list = [0.01, 0.1, 0, 1, 10, 100]\n",
        "train_lambda = []\n",
        "dev_lambda = []\n",
        "\n",
        "# Calcuate the number of parameter updates\n",
        "step = 1\n",
        "############ 計算 training 時間\n",
        "# -*- coding: utf-8 -*-\n",
        "import time\n",
        "tStart = time.time() # 計時測量 training 開始\n",
        "# Iterative training\n",
        "for l in lambda_list: # 運用不同 lambda\n",
        "    train_loss = []\n",
        "    dev_loss = []\n",
        "    train_acc = []\n",
        "    dev_acc = []\n",
        "    w = np.ones((data_dim,)) # 產生 data_dim*1 的 one array\n",
        "    b = np.ones((1,)) # 產生 1*1 的 one array\n",
        "    for epoch in range(max_iter):\n",
        "        # Random shuffle at the begging of each epoch\n",
        "        X_train, Y_train = _shuffle(X_train, Y_train)\n",
        "            \n",
        "        # Mini-batch training\n",
        "        for idx in range(int(np.floor(train_size / batch_size))):\n",
        "            X = X_train[idx*batch_size:(idx+1)*batch_size]\n",
        "            Y = Y_train[idx*batch_size:(idx+1)*batch_size]\n",
        "\n",
        "            # Compute the gradient\n",
        "            w_grad, b_grad = _gradient(X, Y, w, b)\n",
        "            # w_grad, b_grad =  _gradient_L1(X, Y, w, b, l)\n",
        "            # w_grad, b_grad =  _gradient_L2(X, Y, w, b, l)\n",
        "                \n",
        "            # gradient descent update\n",
        "            # learning rate decay with time\n",
        "            w = w - learning_rate/np.sqrt(step) * w_grad\n",
        "            b = b - learning_rate/np.sqrt(step) * b_grad\n",
        "\n",
        "            step = step + 1\n",
        "                \n",
        "        # Compute loss and accuracy of training set and development set\n",
        "        y_train_pred = _f(X_train, w, b)\n",
        "        Y_train_pred = np.round(y_train_pred)\n",
        "        train_acc.append(_accuracy(Y_train_pred, Y_train))\n",
        "        train_loss.append(_cross_entropy_loss(y_train_pred, Y_train) / train_size)\n",
        "\n",
        "        y_dev_pred = _f(X_dev, w, b)\n",
        "        Y_dev_pred = np.round(y_dev_pred)\n",
        "        dev_acc.append(_accuracy(Y_dev_pred, Y_dev))\n",
        "        dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev) / dev_size)\n",
        "        print('epoch =', epoch, ' ,train_acc =', _accuracy(Y_train_pred, Y_train), ' ,dev_acc =', _accuracy(Y_dev_pred, Y_dev))\n",
        "    \n",
        "    train_lambda.append(train_acc)\n",
        "    dev_lambda.append(dev_acc)\n",
        "    # end of 要測量的training\n",
        "    tEnd = time.time() #計時結束\n",
        "    #列印結果\n",
        "    print('Training time is '+str(tEnd - tStart)) #顯示 training 時間\n",
        "    print('Training loss: {}'.format(train_loss[-1]))\n",
        "    print('Development loss: {}'.format(dev_loss[-1]))\n",
        "    print('Training accuracy: {}'.format(train_acc[-1]))\n",
        "    print('Development accuracy: {}'.format(dev_acc[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJuoQ_R2jUmX",
        "colab_type": "text"
      },
      "source": [
        "### Plotting Loss and accuracy curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH3AJtvHjVJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loss curve\n",
        "plt.plot(train_loss)\n",
        "plt.plot(dev_loss)\n",
        "plt.title('Loss')\n",
        "plt.legend(['train', 'dev'])\n",
        "plt.savefig('loss.png')\n",
        "plt.show()\n",
        "\n",
        "# Accuracy curve\n",
        "plt.plot(train_acc)\n",
        "plt.plot(dev_acc)\n",
        "plt.title('Logistic_Accuracy')\n",
        "plt.legend(['train', 'dev'])\n",
        "plt.savefig('acc.png')\n",
        "plt.show()\n",
        "\n",
        "# # show the L1 regularization image\n",
        "# # L1_train Accuracy curve\n",
        "# for i in range(6):\n",
        "#     plt.plot(train_lambda[i][:200])\n",
        "# plt.title('L1 Training Accuracy')\n",
        "# plt.legend(['Lambda: 0.01', 'Lambda: 0.1', 'Lambda: 0', 'Lambda: 1', 'Lambda: 10', 'Lambda: 100'])\n",
        "# plt.savefig('acc.png')\n",
        "# plt.show()\n",
        "\n",
        "# # L1_dev Accuracy curve\n",
        "# for i in range(6):\n",
        "#     plt.plot(dev_lambda[i][:200])\n",
        "# plt.title('L1 Dev Accuracy')\n",
        "# plt.legend(['Lambda: 0.01', 'Lambda: 0.1', 'Lambda: 0', 'Lambda: 1', 'Lambda: 10', 'Lambda: 100'])\n",
        "# plt.savefig('acc.png')\n",
        "# plt.show()\n",
        "\n",
        "# show the L2 regularization image\n",
        "# # L2_train Accuracy curve \n",
        "# for i in range(6):\n",
        "#     plt.plot(train_lambda[i][:200])\n",
        "# plt.title('L2 Training Accuracy')\n",
        "# plt.legend(['Lambda: 0.01', 'Lambda: 0.1', 'Lambda: 0', 'Lambda: 1', 'Lambda: 10', 'Lambda: 100'])\n",
        "# plt.savefig('acc.png')\n",
        "# plt.show()\n",
        "\n",
        "# # L2_dev Accuracy curve \n",
        "# for i in range(6):\n",
        "#     plt.plot(dev_lambda[i][:200])\n",
        "# plt.title('L2 Dev Accuracy')\n",
        "# plt.legend(['Lambda: 0.01', 'Lambda: 0.1', 'Lambda: 0', 'Lambda: 1', 'Lambda: 10', 'Lambda: 100'])\n",
        "# plt.savefig('acc.png')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzIcYAfvkUZ_",
        "colab_type": "text"
      },
      "source": [
        "###Predicting testing labels\n",
        "\n",
        "Predictions are saved to *output_logistic.csv*.\n",
        "\n",
        "預測測試集的資料標籤並且存在 *output_logistic.csv* 中。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEAKhugPkUyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict testing labels\n",
        "predictions = _predict(X_test, w, b)\n",
        "with open(output_fpath.format('logistic'), 'w') as f:\n",
        "    f.write('id,label\\n')\n",
        "    for i, label in  enumerate(predictions):\n",
        "        f.write('{},{}\\n'.format(i, label))\n",
        "# Print out the most significant weights\n",
        "ind = np.argsort(np.abs(w))[::-1] # np.argsort: 返回數組值從小到大的索引值，經過 [::-1] 順序將從大至小\n",
        "with open(X_test_fpath) as f:\n",
        "    content = f.readline().strip('\\n').split(',')\n",
        "features = np.array(content)\n",
        "for i in ind[0:10]:\n",
        "    print(features[i], w[i])\n",
        "\n",
        "#存到本機端\n",
        "# from google.colab import files\n",
        "# files.download('output_logistic.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C6sqhUbLMGe",
        "colab_type": "text"
      },
      "source": [
        "# Porbabilistic generative model\n",
        "\n",
        "In this section we will discuss a generative approach to binary classification. Again, we will not go through the formulation detailedly. Please find [Prof. Lee's lecture](https://www.youtube.com/watch?v=fZAZUYEeIMg) if you are interested in it.\n",
        "\n",
        "接者我們將實作基於 generative model 的二元分類器，理論細節請參考[李宏毅老師的教學影片](https://www.youtube.com/watch?v=fZAZUYEeIMg)。\n",
        "\n",
        "### Preparing Data\n",
        "\n",
        "Training and testing data is loaded and normalized as in logistic regression. However, since LDA is a deterministic algorithm, there is no need to build a development set.\n",
        "\n",
        "訓練集與測試集的處理方法跟 logistic regression 一模一樣，然而因為 generative model 有可解析的最佳解，因此不必使用到 development set。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czWXO7qML8DU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parse csv files to numpy array\n",
        "with open(X_train_fpath) as f:\n",
        "    next(f)\n",
        "    X_train = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\n",
        "with open(Y_train_fpath) as f:\n",
        "    next(f)\n",
        "    Y_train = np.array([line.strip('\\n').split(',')[1] for line in f], dtype = float)\n",
        "with open(X_test_fpath) as f:\n",
        "    next(f)\n",
        "    X_test = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\n",
        "\n",
        "# Normalize training and testing data\n",
        "X_train, X_mean, X_std = _normalize(X_train, train = True)\n",
        "X_test, _, _= _normalize(X_test, train = False, specified_column = None, X_mean = X_mean, X_std = X_std)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8NGKl-EPvok",
        "colab_type": "text"
      },
      "source": [
        "### Mean and Covariance\n",
        "\n",
        "In generative model, in-class mean and covariance are needed.\n",
        "\n",
        "在 generative model 中，我們需要分別計算兩個類別內的資料平均與共變異。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQrzXXKUPwHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute in-class mean\n",
        "X_train_0 = np.array([x for x, y in zip(X_train, Y_train) if y == 0])\n",
        "X_train_1 = np.array([x for x, y in zip(X_train, Y_train) if y == 1])\n",
        "\n",
        "mean_0 = np.mean(X_train_0, axis = 0)\n",
        "mean_1 = np.mean(X_train_1, axis = 0)  \n",
        "\n",
        "# Compute in-class covariance\n",
        "cov_0 = np.zeros((data_dim, data_dim))\n",
        "cov_1 = np.zeros((data_dim, data_dim))\n",
        "\n",
        "for x in X_train_0:\n",
        "    cov_0 += np.dot(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[0]\n",
        "for x in X_train_1:\n",
        "    # cov_1 += np.dot(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[0] #試試看用相同的 cov 此時結果不是理想 (可參考老師 PPT)\n",
        "    cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[0]\n",
        "\n",
        "# Shared covariance is taken as a weighted average of individual in-class covariance.\n",
        "cov = (cov_0 * X_train_0.shape[0] + cov_1 * X_train_1.shape[0]) / (X_train_0.shape[0] + X_train_1.shape[0])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kifW1pFxXXA5",
        "colab_type": "text"
      },
      "source": [
        "### Computing weights and bias\n",
        "\n",
        "Directly compute weights and bias from in-class mean and shared variance. [Prof. Lee's lecture slides](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Classification%20(v3).pdf)(p.33) gives a concise explanation.\n",
        "\n",
        "權重矩陣與偏差向量可以直接被計算出來，算法可以參考[李宏毅老師教學投影片](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Classification%20(v3).pdf)第 33 頁。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UghOxYrUXXPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute inverse of covariance matrix.\n",
        "# Since covariance matrix may be nearly singular, np.linalg.inv() may give a large numerical error.\n",
        "# Via SVD decomposition, one can get matrix inverse efficiently and accurately.\n",
        "u, s, v = np.linalg.svd(cov, full_matrices=False)\n",
        "\n",
        "# 函數：np.linalg.svd(a, full_matrices = 1, compute_uv = 1)。\n",
        "# 參數介紹：\n",
        "# a是一個形如(M,N)矩陣\n",
        "# full_matrices的取值是為 0 或者 1，默認值為 1，這時 u 的大小為 (M,M)，v 的大小為 (N,N)。否則 u 的大小為 (M,K)，v 的大小為 (K,N)，K = min(M,N)。\n",
        "# compute_uv 的取值是為 0 或者 1 ，默認值為 1，表示計算 u, s, v。為 0 的時候只計算 s。\n",
        "\n",
        "# 返回值：\n",
        "# 總共有三個返回值 u,s,v\n",
        "# u 大小為 (M,M)，s大小為 (M,N)，v 大小為 (N,N)。\n",
        "# A = u*s*v\n",
        "# 其中 s 是對矩陣 a 的奇異值分解。 s 除了對角元素不為 0，其他元素都為 0，並且對角元素從大到小排列。 \n",
        "# s 中有 n 個奇異值，一般排在後面的比較接近 0，所以僅保留比較大的 r 個奇異值。\n",
        "\n",
        "inv = np.matmul(v.T * 1 / s, u.T)\n",
        "\n",
        "# Directly compute weights and bias\n",
        "w = np.dot(inv, mean_0 - mean_1)\n",
        "b =  (-0.5) * np.dot(mean_0, np.dot(inv, mean_0)) + 0.5 * np.dot(mean_1, np.dot(inv, mean_1))\\\n",
        "    + np.log(float(X_train_0.shape[0]) / X_train_1.shape[0]) \n",
        "\n",
        "# Compute accuracy on training set\n",
        "Y_train_pred = 1 - _predict(X_train, w, b)\n",
        "print('Training accuracy: {}'.format(_accuracy(Y_train_pred, Y_train)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDKWzBy0bi3c",
        "colab_type": "text"
      },
      "source": [
        "###Predicting testing labels\n",
        "\n",
        "Predictions are saved to *output_generative.csv*.\n",
        "\n",
        "預測測試集的資料標籤並且存在 *output_generative.csv* 中。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T3QjToT_Sq9J",
        "colab": {}
      },
      "source": [
        "# Predict testing labels\n",
        "predictions = 1 - _predict(X_test, w, b)\n",
        "with open(output_fpath.format('generative'), 'w') as f:\n",
        "    f.write('id,label\\n')\n",
        "    for i, label in  enumerate(predictions):\n",
        "        f.write('{},{}\\n'.format(i, label))\n",
        "\n",
        "# Print out the most significant weights\n",
        "ind = np.argsort(np.abs(w))[::-1]\n",
        "with open(X_test_fpath) as f:\n",
        "    content = f.readline().strip('\\n').split(',')\n",
        "features = np.array(content)\n",
        "for i in ind[0:10]:\n",
        "    print(features[i], w[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}