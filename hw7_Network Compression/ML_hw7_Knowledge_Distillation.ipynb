{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_hw7_Knowledge_Distillation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5cFq_TgWlQ_",
        "colab_type": "text"
      },
      "source": [
        "# Homework 7 - Network Compression (Knowledge Distillation)\n",
        "\n",
        "> Author: Arvin Liu (b05902127@ntu.edu.tw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVt1wl936mOS",
        "colab_type": "text"
      },
      "source": [
        "## **goal**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ruf4amHoEDKZ",
        "colab_type": "text"
      },
      "source": [
        " ----- strong baseline -----   0.84100\n",
        "\n",
        "----- simple baseline -----   0.83682"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpmQUZhukmqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download dataset\n",
        "!gdown --id '1GzukFVznTp_RG7b2ury7hr9TwA-MyMYj' --output food-11.zip\n",
        "# Unzip the files\n",
        "!unzip food-11.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSKgt-P7ZhoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# 固定隨機種子\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "    np.random.seed(seed)  # Numpy module.\n",
        "    random.seed(seed)  # Python random module.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "same_seeds(0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNiZCGrIYKdR",
        "colab_type": "text"
      },
      "source": [
        "# Readme\n",
        "\n",
        "\n",
        "HW7的任務是模型壓縮 - Neural Network Compression。\n",
        "\n",
        "Compression有很多種門派，在這裡我們會介紹上課出現過的其中四種，分別是:\n",
        "\n",
        "* 知識蒸餾 Knowledge Distillation\n",
        "* 網路剪枝 Network Pruning\n",
        "* 用少量參數來做CNN Architecture Design\n",
        "* 參數量化 Weight Quantization\n",
        "\n",
        "在這個notebook中我們會介紹Knowledge Distillation，\n",
        "而我們有提供已經學習好的大model方便大家做Knowledge Distillation。\n",
        "而我們使用的小model是\"Architecture Design\"過的model。\n",
        "\n",
        "* Architecute Design在同目錄中的hw7_Architecture_Design.ipynb。\n",
        "* 下載pretrained大model(47.2M): https://drive.google.com/file/d/1B8ljdrxYXJsZv2vmTequdPOofp3VF3NN/view?usp=sharing\n",
        "  * 請使用torchvision提供的ResNet18，把num_classes改成11後load進去即可。(後面有範例。)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdzskhdEb65Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "a463d966-8e3f-4e6a-d6da-02c57262c972"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load進我們的Model架構(在hw7_Architecture_Design.ipynb內) TA_Student_Net\n",
        "!gdown --id '1lJS0ApIyi7eZ2b3GMyGxjPShI8jXM2UC' --output \"hw7_Architecture_Design.ipynb\"\n",
        "%run \"hw7_Architecture_Design.ipynb\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lJS0ApIyi7eZ2b3GMyGxjPShI8jXM2UC\n",
            "To: /content/hw7_Architecture_Design.ipynb\n",
            "\r  0% 0.00/8.78k [00:00<?, ?B/s]\r100% 8.78k/8.78k [00:00<00:00, 14.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdUtCxBBcH0B",
        "colab_type": "text"
      },
      "source": [
        "Knowledge Distillation\n",
        "===\n",
        "\n",
        "<img src=\"https://i.imgur.com/H2aF7Rv.png=100x\" width=\"500px\">\n",
        "\n",
        "簡單上來說就是讓已經做得很好的大model們去告訴小model\"如何\"學習。\n",
        "而我們如何做到這件事情呢? 就是利用大model預測的logits給小model當作標準就可以了。\n",
        "\n",
        "## 為甚麼這會work?\n",
        "* 例如當data不是很乾淨的時候，對一般的model來說他是個noise，只會干擾學習。透過去學習其他大model預測的logits會比較好。\n",
        "* label和label之間可能有關連，這可以引導小model去學習。例如數字8可能就和6,9,0有關係。\n",
        "* 弱化已經學習不錯的target(?)，避免讓其gradient干擾其他還沒學好的task。\n",
        "\n",
        "\n",
        "## 要怎麼實作?\n",
        "* $Loss = \\alpha T^2 \\times KL(\\frac{\\text{Teacher's Logits}}{T} || \\frac{\\text{Student's Logits}}{T}) + (1-\\alpha)(\\text{原本的Loss})$\n",
        "\n",
        "\n",
        "* 以下code為甚麼要對student使用log_softmax: https://github.com/peterliht/knowledge-distillation-pytorch/issues/2\n",
        "* reference: [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-dSi_P-4les",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn_kd(outputs, labels, teacher_outputs, T=20, alpha=0.5):\n",
        "    # 一般的Cross Entropy\n",
        "    hard_loss = F.cross_entropy(outputs, labels) * (1. - alpha)\n",
        "    # 讓logits的log_softmax對目標機率(teacher的logits/T後softmax)做KL Divergence。\n",
        "    soft_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(outputs/T, dim=1),\n",
        "                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T)\n",
        "    return hard_loss + soft_loss"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfnRoOt5VIze",
        "colab_type": "text"
      },
      "source": [
        "# Data Processing\n",
        "\n",
        "我們的Dataset使用的是跟Hw3 - CNN同樣的Dataset，因此這個區塊的Augmentation / Read Image大家參考或直接抄就好。\n",
        "\n",
        "如果有不會的話可以回去看Hw3的colab。\n",
        "\n",
        "需要注意的是如果要自己寫的話，Augment的方法最好使用我們的方法，避免輸入有差異導致Teacher Net預測不好。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExdUvTRaVNOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import torch\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, folderName, transform=None):\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "        self.label = []\n",
        "\n",
        "        for img_path in sorted(glob(folderName + '/*.jpg')):\n",
        "            try:\n",
        "                # Get classIdx by parsing image path\n",
        "                class_idx = int(re.findall(re.compile(r'\\d+'), img_path)[1])\n",
        "            except:\n",
        "                # if inference mode (there's no answer), class_idx default 0\n",
        "                class_idx = 0\n",
        "\n",
        "            image = Image.open(img_path)\n",
        "            # Get File Descriptor\n",
        "            image_fp = image.fp\n",
        "            image.load()\n",
        "            # Close File Descriptor (or it'll reach OPEN_MAX)\n",
        "            image_fp.close()\n",
        "\n",
        "            self.data.append(image)\n",
        "            self.label.append(class_idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        image = self.data[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, self.label[idx]\n",
        "\n",
        "\n",
        "trainTransform = transforms.Compose([\n",
        "    transforms.RandomCrop(256, pad_if_needed=True, padding_mode='symmetric'),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "testTransform = transforms.Compose([\n",
        "    transforms.CenterCrop(256),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "def get_dataloader(mode='training', batch_size=32):\n",
        "\n",
        "    assert mode in ['training', 'testing', 'validation']\n",
        "\n",
        "    dataset = MyDataset(\n",
        "        f'./food-11/{mode}', #原本的\n",
        "        # f'./{mode}', #之前發現 zip 沒 folder\n",
        "        transform=trainTransform if mode == 'training' else testTransform)\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=(mode == 'training'))\n",
        "\n",
        "    return dataloader\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACPwL9_JWceQ",
        "colab_type": "text"
      },
      "source": [
        "# Pre-processing\n",
        "\n",
        "我們已經提供TeacherNet的state_dict，其架構是torchvision提供的ResNet18。\n",
        "\n",
        "至於StudentNet的架構則在hw7_Architecture_Design.ipynb中。\n",
        "\n",
        "這裡我們使用的Optimizer為AdamW，沒有為甚麼，就純粹我想用。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzuuGvnbWkG8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ffa2f248-1cb8-4533-e708-4ef10b7b5a6d"
      },
      "source": [
        "# get dataloader\n",
        "train_dataloader = get_dataloader('training', batch_size=32)\n",
        "print('finish train_dataloader')\n",
        "\n",
        "valid_dataloader = get_dataloader('validation', batch_size=32)\n",
        "print('finish valid_dataloader')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "finish train_dataloader\n",
            "finish valid_dataloader\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWdQtDtgoGCp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "8d6c09dd-e24b-4cb9-82f3-e8d9207c5258"
      },
      "source": [
        "# # 使用 pre-train teacher_resnet18.bin\n",
        "# !gdown --id '1B8ljdrxYXJsZv2vmTequdPOofp3VF3NN' --output teacher_resnet18.bin\n",
        "\n",
        "# # 使用 pre-train teacher_resnet18.bin\n",
        "# !gdown --id '1xiaRepwfa1XOwwKswzbHfXaDmBEVpZVh' --output teacher_resnet18.bin #preatrain\n",
        "\n",
        "# 使用 teacher_resnet18_from_scratch.bin\n",
        "!gdown --id '1VEoKts_clMcJYKnuUdxcNX1BtPcPTmsc' --output teacher_resnet18.bin #from scratch\n",
        "\n",
        "teacher_net = models.resnet18(pretrained=False, num_classes=11).cuda() #把 teacher net load\n",
        "student_net = StudentNet(base=16).cuda()\n",
        "\n",
        "teacher_net.load_state_dict(torch.load(f'./teacher_resnet18.bin'))\n",
        "optimizer = optim.AdamW(student_net.parameters(), lr=1e-3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VEoKts_clMcJYKnuUdxcNX1BtPcPTmsc\n",
            "To: /content/teacher_resnet18.bin\n",
            "44.8MB [00:00, 96.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvc1W5yO2QaE",
        "colab_type": "text"
      },
      "source": [
        "# Start Training\n",
        "\n",
        "* 剩下的步驟與你在做Hw3 - CNN的時候一樣。\n",
        "\n",
        "## 小提醒\n",
        "\n",
        "* torch.no_grad是指接下來的運算或該tensor不需要算gradient。\n",
        "* model.eval()與model.train()差在於Batchnorm要不要紀錄，以及要不要做Dropout。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGpNUgDIWFf5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "b71e3823-b40d-4436-d3c0-88e8bb0e7705"
      },
      "source": [
        "# # 拿之前的再 train\n",
        "# !gdown --id '110YMEwwIyLNJNcaQ_uV_0daewsNz3Wiy' --output student_custom_small.bin\n",
        "!gdown --id '10c_s5vehB1B0zyQfT-tnBqxmETB6bJjY' --output student_custom_small.bin\n",
        "\n",
        "student_net = StudentNet(base=16).cuda()\n",
        "student_net.load_state_dict(torch.load('student_custom_small.bin'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10c_s5vehB1B0zyQfT-tnBqxmETB6bJjY\n",
            "To: /content/student_custom_small.bin\n",
            "\r  0% 0.00/1.05M [00:00<?, ?B/s]\r100% 1.05M/1.05M [00:00<00:00, 72.3MB/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TzmWtT62Qmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 先給他跑 170 次\n",
        "# 已經 40 + 50 + 60\n",
        "# 如果 lr too small 繼續加\n",
        "\n",
        "optimizer = optim.AdamW(student_net.parameters(), lr=1e-3)\n",
        "import time\n",
        "def run_epoch(dataloader, update=True, alpha=0.5):\n",
        "    total_num, total_hit, total_loss = 0, 0, 0\n",
        "    for now_step, batch_data in enumerate(dataloader):\n",
        "        # 清空 optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # 處理 input\n",
        "        inputs, hard_labels = batch_data\n",
        "        inputs = inputs.cuda()\n",
        "        hard_labels = torch.LongTensor(hard_labels).cuda()\n",
        "        # 因為Teacher沒有要backprop，所以我們使用torch.no_grad\n",
        "        # 告訴torch不要暫存中間值(去做backprop)以浪費記憶體空間。\n",
        "        with torch.no_grad():\n",
        "            soft_labels = teacher_net(inputs)\n",
        "\n",
        "        if update:\n",
        "            logits = student_net(inputs)\n",
        "            # 使用我們之前所寫的融合soft label&hard label的loss。\n",
        "            # T=20是原始論文的參數設定。\n",
        "            loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)\n",
        "            loss.backward()\n",
        "            optimizer.step()    \n",
        "        else:\n",
        "            # 只是算validation acc的話，就開no_grad節省空間。\n",
        "            with torch.no_grad():\n",
        "                logits = student_net(inputs)\n",
        "                loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)\n",
        "            \n",
        "        total_hit += torch.sum(torch.argmax(logits, dim=1) == hard_labels).item()\n",
        "        total_num += len(inputs)\n",
        "\n",
        "        total_loss += loss.item() * len(inputs)\n",
        "    return total_loss / total_num, total_hit / total_num\n",
        "\n",
        "\n",
        "# TeacherNet永遠都是Eval mode.\n",
        "teacher_net.eval()\n",
        "now_best_acc = 0\n",
        "for epoch in range(170):\n",
        "    train_start_time = time.time()\n",
        "    student_net.train()\n",
        "    train_loss, train_acc = run_epoch(train_dataloader, update=True)\n",
        "    student_net.eval()\n",
        "    valid_loss, valid_acc = run_epoch(valid_dataloader, update=False)\n",
        "\n",
        "    # 存下最好的model。\n",
        "    if valid_acc > now_best_acc:\n",
        "        now_best_acc = valid_acc\n",
        "        torch.save(student_net.state_dict(), 'student_model.bin')\n",
        "        print('save model')\n",
        "    print('epoch {:>3d}: train loss: {:6.4f}, acc {:6.4f} valid loss: {:6.4f}, acc {:6.4f}'.format(\n",
        "        epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
        "    print('epoch cost time =', time.time() - train_start_time)\n",
        "    print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzCb6Zf_19Ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # 拿之前的再 train\n",
        "!gdown --id '110YMEwwIyLNJNcaQ_uV_0daewsNz3Wiy' --output student_custom_small.bin\n",
        "\n",
        "student_net = StudentNet(base=16).cuda()\n",
        "student_net.load_state_dict(torch.load('student_custom_small.bin'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvQEEHdm8pBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 重跑 30 次\n",
        "\n",
        "optimizer = optim.AdamW(student_net.parameters(), lr=1e-4)\n",
        "\n",
        "import time\n",
        "def run_epoch(dataloader, update=True, alpha=0.5):\n",
        "    total_num, total_hit, total_loss = 0, 0, 0\n",
        "    for now_step, batch_data in enumerate(dataloader):\n",
        "        # 清空 optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # 處理 input\n",
        "        inputs, hard_labels = batch_data\n",
        "        inputs = inputs.cuda()\n",
        "        hard_labels = torch.LongTensor(hard_labels).cuda()\n",
        "        # 因為Teacher沒有要backprop，所以我們使用torch.no_grad\n",
        "        # 告訴torch不要暫存中間值(去做backprop)以浪費記憶體空間。\n",
        "        with torch.no_grad():\n",
        "            soft_labels = teacher_net(inputs)\n",
        "\n",
        "        if update:\n",
        "            logits = student_net(inputs)\n",
        "            # 使用我們之前所寫的融合soft label&hard label的loss。\n",
        "            # T=20是原始論文的參數設定。\n",
        "            loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)\n",
        "            loss.backward()\n",
        "            optimizer.step()    \n",
        "        else:\n",
        "            # 只是算validation acc的話，就開no_grad節省空間。\n",
        "            with torch.no_grad():\n",
        "                logits = student_net(inputs)\n",
        "                loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)\n",
        "            \n",
        "        total_hit += torch.sum(torch.argmax(logits, dim=1) == hard_labels).item()\n",
        "        total_num += len(inputs)\n",
        "\n",
        "        total_loss += loss.item() * len(inputs)\n",
        "    return total_loss / total_num, total_hit / total_num\n",
        "\n",
        "\n",
        "# TeacherNet永遠都是Eval mode.\n",
        "teacher_net.eval()\n",
        "now_best_acc = 0\n",
        "for epoch in range(30):\n",
        "    train_start_time = time.time()\n",
        "    student_net.train()\n",
        "    train_loss, train_acc = run_epoch(train_dataloader, update=True)\n",
        "    student_net.eval()\n",
        "    valid_loss, valid_acc = run_epoch(valid_dataloader, update=False)\n",
        "\n",
        "    # 存下最好的model。\n",
        "    if valid_acc > now_best_acc:\n",
        "        now_best_acc = valid_acc\n",
        "        torch.save(student_net.state_dict(), 'student_model.bin')\n",
        "        print('save model')\n",
        "    print('epoch {:>3d}: train loss: {:6.4f}, acc {:6.4f} valid loss: {:6.4f}, acc {:6.4f}'.format(\n",
        "        epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
        "    print('epoch cost time =', time.time() - train_start_time)\n",
        "    print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71pof2H7avOf",
        "colab_type": "text"
      },
      "source": [
        "看 StudentNet 參數量\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJTaZXE5awCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(student_net, input_size=(3, 128, 128))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFgstoF--SEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_parameter_number(net):\n",
        "    total_num = sum(p.numel() for p in net.parameters())\n",
        "    trainable_num = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
        "    return {'Total': total_num, 'Trainable': trainable_num}"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBVaoR6R9vrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d7f59e8c-27c3-42ff-97b0-13da3cef8921"
      },
      "source": [
        "get_parameter_number(student_net)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Total': 256779, 'Trainable': 256779}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7ryPFyna2JY",
        "colab_type": "text"
      },
      "source": [
        "看 teacher_net 參數量"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7vwhKCFbDwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(teacher_net, input_size=(3, 128, 128))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GObCiGNtPkZ",
        "colab_type": "text"
      },
      "source": [
        "# Inference\n",
        "\n",
        "同Hw3，請參考該作業:)。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TapJ1t54dB_W",
        "colab_type": "text"
      },
      "source": [
        "## testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icaej4ooae6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 讀 train 好的檔\n",
        "# !gdown --id '10usrlxc7KhTbwRTzG7IAmaFbsVdWqlQ3' --output student_custom_small.bin #predict_0.8402332361516035_student_model.csv\n",
        "!gdown --id '11MtUk-wHWrV004j9Li-bsQFfWB2Kkl3j' --output student_custom_small.bin #predict_0.8131195335276968student_model.csv\n",
        "\n",
        "student_net = StudentNet(base=16).cuda()\n",
        "student_net.load_state_dict(torch.load('student_custom_small.bin'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6z06ODSdIGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataloader = get_dataloader('testing', batch_size=32)\n",
        "print('finish test_dataloader')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8sNKxjqhPDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "student_net.eval()\n",
        "prediction = []\n",
        "\n",
        "for now_step, batch_data in enumerate(test_dataloader):\n",
        "    # 清空 optimizer\n",
        "    # optimizer.zero_grad()\n",
        "    # 處理 input\n",
        "    inputs, hard_labels = batch_data\n",
        "    inputs = inputs.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = student_net(inputs)\n",
        "        test_label = np.argmax(logits.cpu().data.numpy(), axis=1)\n",
        "        for y in test_label:\n",
        "            prediction.append(y)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EXQdnLLdv9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 丟到 hw7\n",
        "from google.colab import files\n",
        "\n",
        "#將結果寫入 csv 檔\n",
        "with open(\"hw7_predict_0.8131195335276968student_model.csv\", 'w') as f:\n",
        "    f.write('Id,label\\n')\n",
        "    for i, y in  enumerate(prediction):\n",
        "        f.write('{},{}\\n'.format(i, y))\n",
        "#存到本機端\n",
        "files.download(\"hw7_predict_0.8131195335276968student_model.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZGX5hEwJuez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 丟到 hw3\n",
        "# 可以使用 hw3 kaggle predict\n",
        "from google.colab import files\n",
        "\n",
        "#將結果寫入 csv 檔\n",
        "with open(\"hw3_predict_0.8131195335276968student_model.csv\", 'w') as f:\n",
        "    f.write('Id,Category\\n')\n",
        "    for i, y in  enumerate(prediction):\n",
        "        f.write('{},{}\\n'.format(i, y))\n",
        "#存到本機端\n",
        "files.download(\"hw3_predict_0.8131195335276968student_model.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d4Vtg60ed-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Kaggle Score Record\n",
        "\n",
        "# 1. predict_TA.csv\n",
        "#   acc = 0.82964\n",
        "\n",
        "# 2. predict_0.8440233236151603_student_model.csv\n",
        "#   acc = 0.04064\n",
        "#   https://drive.google.com/open?id=10uOiw6Hsn0dYQe9TnNpxGaJVp4V6QVbt\n",
        "\n",
        "# 3. predict_0.8402332361516035_student_model.csv\n",
        "#   acc = 0.86072\n",
        "#   https://drive.google.com/open?id=10usrlxc7KhTbwRTzG7IAmaFbsVdWqlQ3\n",
        "\n",
        "# 4. predict_0.8402332361516035_student_model_8bytes.csv\n",
        "#   acc = 0.85475\n",
        "#   https://drive.google.com/open?id=10usrlxc7KhTbwRTzG7IAmaFbsVdWqlQ3\n",
        "\n",
        "# 5. predict_0.8131195335276968student_model.csv\n",
        "#   acc = 0.83024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIcblvbUCTOP",
        "colab_type": "text"
      },
      "source": [
        "# Q&A\n",
        "\n",
        "有任何問題Network Compression的問題可以寄信到b05902127@ntu.edu.tw / ntu-ml-2020spring-ta@googlegroups.com。\n",
        "\n",
        "時間允許的話我會更新在這裡。"
      ]
    }
  ]
}