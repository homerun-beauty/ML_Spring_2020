{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_hw7_Network_Pruning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5cFq_TgWlQ_",
        "colab_type": "text"
      },
      "source": [
        "# Homework 7 - Network Compression (Network Pruning)\n",
        "\n",
        "> Author: Arvin Liu (b05902127@ntu.edu.tw)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "205MwcXRadvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder_path = './'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpmQUZhukmqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download dataset\n",
        "!gdown --id '1GzukFVznTp_RG7b2ury7hr9TwA-MyMYj' --output food-11.zip\n",
        "# Unzip the files\n",
        "!unzip food-11.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bauDTAk0ZmP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# 固定隨機種子\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "    np.random.seed(seed)  # Numpy module.\n",
        "    random.seed(seed)  # Python random module.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "same_seeds(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNiZCGrIYKdR",
        "colab_type": "text"
      },
      "source": [
        "# Readme\n",
        "\n",
        "\n",
        "HW7的任務是模型壓縮 - Neural Network Compression。\n",
        "\n",
        "Compression有很多種門派，在這裡我們會介紹上課出現過的其中四種，分別是:\n",
        "\n",
        "* 知識蒸餾 Knowledge Distillation\n",
        "* 網路剪枝 Network Pruning\n",
        "* 用少量參數來做CNN Architecture Design\n",
        "* 參數量化 Weight Quantization\n",
        "\n",
        "在這個notebook中我們會介紹Network Pruning，\n",
        "而我們有提供已經做完Knowledge Distillation的小model來做Pruning。\n",
        "\n",
        "* Model架構 / Architecute Design在同目錄中的hw7_Architecture_Design.ipynb。\n",
        "* 下載已經train好的小model(0.99M): https://drive.google.com/open?id=12wtIa0WVRcpboQzhgRUJOpcXe23tgWUL\n",
        "  * 參數為 base=16, width_mult=1 (default)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdzskhdEb65Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "# # Load進我們的Model架構(在hw7_Architecture_Design.ipynb內)\n",
        "# !gdown --id '1lJS0ApIyi7eZ2b3GMyGxjPShI8jXM2UC' --output \"hw7_Architecture_Design.ipynb\"\n",
        "# %run \"hw7_Architecture_Design.ipynb\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdUtCxBBcH0B",
        "colab_type": "text"
      },
      "source": [
        "Network Pruning\n",
        "===\n",
        "在這裡我們會教Neuron Pruning。\n",
        "<img src=\"https://i.imgur.com/Iwp90Wp.png\" width=\"500px\">\n",
        "\n",
        "簡單上來說就是讓一個已經學完的model中的neuron進行刪減，讓整個網路變得更瘦。\n",
        "\n",
        "## Weight & Neuron Pruning\n",
        "* weight和neuron pruning差別在於prune掉一個neuron就等於是把一個matrix的整個column全部砍掉。但如此一來速度就會比較快。因為neuron pruning後matrix整體變小，但weight pruning大小不變，只是有很多空洞。\n",
        "\n",
        "## What to Prune?\n",
        "* 既然要Neuron Pruning，那就必須要先衡量Neuron的重要性。衡量完所有的Neuron後，就可以把比較不重要的Neuron刪減掉。\n",
        "* 在這裡我們介紹一個很簡單可以衡量Neuron重要性的方法 - 就是看batchnorm layer的$\\gamma$因子來決定neuron的重要性。 (by paper - Network Slimming)\n",
        "  ![](https://i.imgur.com/JVpCm2r.png)\n",
        "* 相信大家看這個pytorch提供的batchnorm公式應該就可以意識到為甚麼$\\gamma$可以當作重要性來衡量了:)\n",
        "\n",
        "* Network Slimming其實步驟沒有這麼簡單，有興趣的同學可以check以下連結。[Netowrk Slimming](https://arxiv.org/abs/1708.06519)\n",
        "\n",
        "\n",
        "## 為甚麼這會 work?\n",
        "* 樹多必有枯枝，有些neuron只是在躺分，所以有他沒他沒有差。\n",
        "* 困難的說可以回想起老師說過的大樂透假說(The Lottery Ticket Hypothesis)就可以知道了。\n",
        "\n",
        "## 要怎麼實作?\n",
        "* 為了避免複雜的操作，我們會將StudentNet(width_mult=$\\alpha$)的neuron經過篩選後移植到StudentNet(width_mult=$\\beta$)。($\\alpha > \\beta$)\n",
        "* 篩選的方法也很簡單，只需要抓出每一個block的batchnorm的$\\gamma$即可。\n",
        "\n",
        "## 一些實作細節\n",
        "* 假設model中間兩層是這樣的:\n",
        "\n",
        "|Layer|Output # of Channels|\n",
        "|-|-|\n",
        "|Input|in_chs|\n",
        "|Depthwise(in_chs)|in_chs|\n",
        "|BatchNorm(in_chs)|in_chs|\n",
        "|Pointwise(in_chs, **mid_chs**)|**mid_chs**|\n",
        "|**Depthwise(mid_chs)**|**mid_chs**|\n",
        "|**BatchNorm(mid_chs)**|**mid_chs**|\n",
        "|Pointwise(**mid_chs**, out_chs)|out_chs|\n",
        "\n",
        "則你會發現利用第二個BatchNorm來做篩選的時候，跟他的Neuron有直接關係的是該層的Depthwise&Pointwise以及上層的Pointwise。\n",
        "因此再做neuron篩選時記得要將這四個(包括自己, bn)也要同時prune掉。\n",
        "\n",
        "* 在Design Architecure內，model的一個block，名稱所對應的Weight；\n",
        "\n",
        "|#|name|meaning|code|weight shape|\n",
        "|-|-|-|-|-|\n",
        "|0|cnn.{i}.0|Depthwise Convolution Layer|nn.Conv2d(x, x, 3, 1, 1, group=x)|(x, 1, 3, 3)|\n",
        "|1|cnn.{i}.1|Batch Normalization|nn.BatchNorm2d(x)|(x)|\n",
        "|2||ReLU6|nn.ReLU6||\n",
        "|3|cnn.{i}.3|Pointwise Convolution Layer|nn.Conv2d(x, y, 1),|(y, x, 1, 1)|\n",
        "|4||MaxPooling|nn.MaxPool2d(2, 2, 0)||"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-dSi_P-4les",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def network_slimming(old_model, new_model):\n",
        "    params = old_model.state_dict()\n",
        "    new_params = new_model.state_dict()\n",
        "    \n",
        "    # selected_idx: 每一層所選擇的neuron index\n",
        "    selected_idx = []\n",
        "    # 我們總共有7層CNN，因此逐一抓取選擇的neuron index們。\n",
        "    for i in range(8):\n",
        "        # 根據上表，我們要抓的gamma係數在cnn.{i}.1.weight內。\n",
        "        importance = params[f'cnn.{i}.1.weight']\n",
        "        # 抓取總共要篩選幾個neuron。\n",
        "        old_dim = len(importance)\n",
        "        new_dim = len(new_params[f'cnn.{i}.1.weight'])\n",
        "        # 以Ranking做Index排序，較大的會在前面(descending=True)。\n",
        "        ranking = torch.argsort(importance, descending=True)\n",
        "        # 把篩選結果放入selected_idx中。\n",
        "        selected_idx.append(ranking[:new_dim])\n",
        "\n",
        "    now_processed = 1\n",
        "    for (name, p1), (name2, p2) in zip(params.items(), new_params.items()):\n",
        "        # 如果是cnn層，則移植參數。\n",
        "        # 如果是FC層，或是該參數只有一個數字(例如batchnorm的tracenum等等資訊)，那麼就直接複製。\n",
        "        if name.startswith('cnn') and p1.size() != torch.Size([]) and now_processed != len(selected_idx):\n",
        "            # 當處理到Pointwise的weight時，讓now_processed+1，表示該層的移植已經完成。\n",
        "            if name.startswith(f'cnn.{now_processed}.3'):\n",
        "                now_processed += 1\n",
        "\n",
        "            # 如果是pointwise，weight會被上一層的pruning和下一層的pruning所影響，因此需要特判。\n",
        "            if name.endswith('3.weight'):\n",
        "                # 如果是最後一層cnn，則輸出的neuron不需要prune掉。\n",
        "                if len(selected_idx) == now_processed:\n",
        "                    new_params[name] = p1[:,selected_idx[now_processed-1]]\n",
        "                # 反之，就依照上層和下層所選擇的index進行移植。\n",
        "                # 這裡需要注意的是Conv2d(x,y,1)的weight shape是(y,x,1,1)，順序是反的。\n",
        "                else:\n",
        "                    new_params[name] = p1[selected_idx[now_processed]][:,selected_idx[now_processed-1]]\n",
        "            else:\n",
        "                new_params[name] = p1[selected_idx[now_processed]]\n",
        "        else:\n",
        "            new_params[name] = p1\n",
        "\n",
        "    # 讓新model load進被我們篩選過的parameters，並回傳new_model。        \n",
        "    new_model.load_state_dict(new_params)\n",
        "    return new_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-wUGvS5uQ1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prune resnet18\n",
        "\n",
        "def network_slimming(old_model, new_model):\n",
        "    params = old_model.state_dict()\n",
        "    new_params = new_model.state_dict()\n",
        "    \n",
        "    # selected_idx: 每一層所選擇的neuron index\n",
        "    selected_idx = []\n",
        "    # 我們總共有20層CNN，因此逐一抓取選擇的neuron index們。\n",
        "    for i in range(20):\n",
        "        # 根據上表，我們要抓的gamma係數在cnn.{i}.1.weight內。\n",
        "        importance = params[f'cnn.{i}.1.weight']\n",
        "        # 抓取總共要篩選幾個neuron。\n",
        "        old_dim = len(importance)\n",
        "        new_dim = len(new_params[f'cnn.{i}.1.weight'])\n",
        "        # 以Ranking做Index排序，較大的會在前面(descending=True)。\n",
        "        ranking = torch.argsort(importance, descending=True)\n",
        "        # 把篩選結果放入selected_idx中。\n",
        "        selected_idx.append(ranking[:new_dim])\n",
        "\n",
        "    now_processed = 1\n",
        "    for (name, p1), (name2, p2) in zip(params.items(), new_params.items()):\n",
        "        # 如果是cnn層，則移植參數。\n",
        "        # 如果是FC層，或是該參數只有一個數字(例如batchnorm的tracenum等等資訊)，那麼就直接複製。\n",
        "        if name.startswith('cnn') and p1.size() != torch.Size([]) and now_processed != len(selected_idx):\n",
        "            # 當處理到Pointwise的weight時，讓now_processed+1，表示該層的移植已經完成。\n",
        "            if name.startswith(f'cnn.{now_processed}.3'):\n",
        "                now_processed += 1\n",
        "\n",
        "            # 如果是pointwise，weight會被上一層的pruning和下一層的pruning所影響，因此需要特判。\n",
        "            if name.endswith('3.weight'):\n",
        "                # 如果是最後一層cnn，則輸出的neuron不需要prune掉。\n",
        "                if len(selected_idx) == now_processed:\n",
        "                    new_params[name] = p1[:,selected_idx[now_processed-1]]\n",
        "                # 反之，就依照上層和下層所選擇的index進行移植。\n",
        "                # 這裡需要注意的是Conv2d(x,y,1)的weight shape是(y,x,1,1)，順序是反的。\n",
        "                else:\n",
        "                    new_params[name] = p1[selected_idx[now_processed]][:,selected_idx[now_processed-1]]\n",
        "            else:\n",
        "                new_params[name] = p1[selected_idx[now_processed]]\n",
        "        else:\n",
        "            new_params[name] = p1\n",
        "\n",
        "    # 讓新model load進被我們篩選過的parameters，並回傳new_model。        \n",
        "    new_model.load_state_dict(new_params)\n",
        "    return new_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfnRoOt5VIze",
        "colab_type": "text"
      },
      "source": [
        "# Data Processing\n",
        "\n",
        "我們的Dataset使用的是跟Hw3 - CNN同樣的Dataset，因此這個區塊的Augmentation / Read Image大家參考就好。\n",
        "\n",
        "如果有不會的話可以回去看Hw3的colab。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExdUvTRaVNOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import torch\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, folderName, transform=None):\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "        self.label = []\n",
        "\n",
        "        for img_path in sorted(glob(folderName + '/*.jpg')):\n",
        "            try:\n",
        "                # Get classIdx by parsing image path\n",
        "                class_idx = int(re.findall(re.compile(r'\\d+'), img_path)[1])\n",
        "            except:\n",
        "                # if inference mode (there's no answer), class_idx default 0\n",
        "                class_idx = 0\n",
        " \n",
        "            image = Image.open(img_path)\n",
        "            # Get File Descriptor\n",
        "            image_fp = image.fp\n",
        "            image.load()\n",
        "            # Close File Descriptor (or it'll reach OPEN_MAX)\n",
        "            image_fp.close()\n",
        "\n",
        "            self.data.append(image)\n",
        "            self.label.append(class_idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        image = self.data[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, self.label[idx]\n",
        "\n",
        "\n",
        "trainTransform = transforms.Compose([\n",
        "    transforms.RandomCrop(256, pad_if_needed=True, padding_mode='symmetric'),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "testTransform = transforms.Compose([\n",
        "    transforms.CenterCrop(256),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "def get_dataloader(mode='training', batch_size=32):\n",
        "\n",
        "    assert mode in ['training', 'testing', 'validation']\n",
        "\n",
        "    dataset = MyDataset(\n",
        "        f'./food-11/{mode}',\n",
        "        transform=trainTransform if mode == 'training' else testTransform)\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=(mode == 'training'))\n",
        "\n",
        "    return dataloader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACPwL9_JWceQ",
        "colab_type": "text"
      },
      "source": [
        "# Pre-processing\n",
        "\n",
        "我們已經提供原始小model binary，架構是hw7_Architecture_Design.ipynb中的StudentNet。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzuuGvnbWkG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get dataloader\n",
        "train_dataloader = get_dataloader('training', batch_size=32)\n",
        "print('finish train_dataloader')\n",
        "\n",
        "valid_dataloader = get_dataloader('validation', batch_size=32)\n",
        "print('finish valid_dataloader')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWdQtDtgoGCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !gdown --id '12wtIa0WVRcpboQzhgRUJOpcXe23tgWUL' --output student_custom_small.bin\n",
        "!gdown --id '1-BVZoTUkX0faW4sYk7L2qiZbo-uYdn0P' --output student_custom_small.bin #使用上去 kaggle 的\n",
        "\n",
        "net = StudentNet().cuda()\n",
        "net.load_state_dict(torch.load('student_custom_small.bin'))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(net.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPqFxalDXbNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 使用 pre-train teacher_resnet18.bin\n",
        "!gdown --id '1B8ljdrxYXJsZv2vmTequdPOofp3VF3NN' --output teacher_resnet18.bin\n",
        "\n",
        "net = models.resnet18(pretrained=False, num_classes=11).cuda() #把 teacher net load\n",
        "\n",
        "net.load_state_dict(torch.load(f'./teacher_resnet18.bin'))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(net.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvc1W5yO2QaE",
        "colab_type": "text"
      },
      "source": [
        "# Start Training\n",
        "\n",
        "* 每次Prune rate是0.95，Prune完後會重新fine-tune 3 epochs。\n",
        "* 其餘的步驟與你在做Hw3 - CNN的時候一樣。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOGLynQOEN9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_parameter_number(net):\n",
        "    # total_num = sum(p.numel() for p in net.parameters())\n",
        "    trainable_num = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
        "    print('trainable parameters =', trainable_num)\n",
        "    return trainable_num"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TzmWtT62Qmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from torchsummary import summary\n",
        "\n",
        "def run_epoch(dataloader, update=True, alpha=0.5):\n",
        "    total_num, total_hit, total_loss = 0, 0, 0\n",
        "    for now_step, batch_data in enumerate(dataloader):\n",
        "        # 清空 optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # 處理 input\n",
        "        inputs, labels = batch_data\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "  \n",
        "        logits = net(inputs)\n",
        "        loss = criterion(logits, labels)\n",
        "        if update:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_hit += torch.sum(torch.argmax(logits, dim=1) == labels).item()\n",
        "        total_num += len(inputs)\n",
        "        total_loss += loss.item() * len(inputs)\n",
        "\n",
        "    return total_loss / total_num, total_hit / total_num\n",
        "\n",
        "\n",
        "parameters_acc_095 = []\n",
        "now_width_mult = 1\n",
        "\n",
        "for i in range(5):\n",
        "    # print(summary(net, input_size=(3, 128, 128)))\n",
        "    print('for i =', i)\n",
        "    now_width_mult *= 0.95\n",
        "    new_net = StudentNet(width_mult=now_width_mult).cuda()\n",
        "    # new_net = models.resnet18(pretrained=False, num_classes=11).cuda()\n",
        "    params = net.state_dict()\n",
        "    net = network_slimming(net, new_net)\n",
        "    now_best_acc = 0\n",
        "    for epoch in range(5):\n",
        "        # print(summary(net, input_size=(3, 128, 128)))\n",
        "        train_start_time = time.time()\n",
        "        net.train()\n",
        "        train_loss, train_acc = run_epoch(train_dataloader, update=True)\n",
        "        net.eval()\n",
        "        valid_loss, valid_acc = run_epoch(valid_dataloader, update=False)\n",
        "        # 在每個width_mult的情況下，存下最好的model。\n",
        "        if valid_acc > now_best_acc:\n",
        "            now_best_acc = valid_acc\n",
        "            torch.save(net.state_dict(), f'custom_small_rate_{now_width_mult}.bin')\n",
        "            print('save model')\n",
        "        print('rate {:6.4f} epoch {:>3d}: train loss: {:6.4f}, acc {:6.4f} valid loss: {:6.4f}, acc {:6.4f}'.format(now_width_mult, \n",
        "            epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
        "        print('epoch cost time =', time.time() - train_start_time)\n",
        "        print('')\n",
        "    parameters_acc_095.append([get_parameter_number(net),now_best_acc])\n",
        "    print('----------------------------------------------------------------------------------------------')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7dcPL9TbCpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net_090 = StudentNet().cuda()\n",
        "net_090.load_state_dict(torch.load('student_custom_small.bin'))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(net_090.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpg-4yRGam9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from torchsummary import summary\n",
        "\n",
        "def run_epoch(dataloader, update=True, alpha=0.5):\n",
        "    total_num, total_hit, total_loss = 0, 0, 0\n",
        "    for now_step, batch_data in enumerate(dataloader):\n",
        "        # 清空 optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # 處理 input\n",
        "        inputs, labels = batch_data\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "  \n",
        "        logits = net_090(inputs)\n",
        "        loss = criterion(logits, labels)\n",
        "        if update:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_hit += torch.sum(torch.argmax(logits, dim=1) == labels).item()\n",
        "        total_num += len(inputs)\n",
        "        total_loss += loss.item() * len(inputs)\n",
        "\n",
        "    return total_loss / total_num, total_hit / total_num\n",
        "\n",
        "\n",
        "parameters_acc_090 = []\n",
        "now_width_mult = 1\n",
        "\n",
        "for i in range(5):\n",
        "    # print(summary(net, input_size=(3, 128, 128)))\n",
        "    print('for i =', i)\n",
        "    now_width_mult *= 0.90\n",
        "    new_net = StudentNet(width_mult=now_width_mult).cuda()\n",
        "    # new_net = models.resnet18(pretrained=False, num_classes=11).cuda()\n",
        "    params = net_090.state_dict()\n",
        "    net_090 = network_slimming(net_090, new_net)\n",
        "    now_best_acc = 0\n",
        "    for epoch in range(5):\n",
        "        # print(summary(net, input_size=(3, 128, 128)))\n",
        "        train_start_time = time.time()\n",
        "        net_090.train()\n",
        "        train_loss, train_acc = run_epoch(train_dataloader, update=True)\n",
        "        net_090.eval()\n",
        "        valid_loss, valid_acc = run_epoch(valid_dataloader, update=False)\n",
        "        # 在每個width_mult的情況下，存下最好的model。\n",
        "        if valid_acc > now_best_acc:\n",
        "            now_best_acc = valid_acc\n",
        "            torch.save(net_090.state_dict(), f'custom_small_rate_{now_width_mult}.bin')\n",
        "            print('save model')\n",
        "        print('rate {:6.4f} epoch {:>3d}: train loss: {:6.4f}, acc {:6.4f} valid loss: {:6.4f}, acc {:6.4f}'.format(now_width_mult, \n",
        "            epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
        "        print('epoch cost time =', time.time() - train_start_time)\n",
        "        print('')\n",
        "    parameters_acc_090.append([get_parameter_number(net_090),now_best_acc])\n",
        "    print('----------------------------------------------------------------------------------------------')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzVoSMwHzX05",
        "colab_type": "text"
      },
      "source": [
        "## check parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CMzz57izDX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(net, input_size=(3, 128, 128))\n",
        "\n",
        "# from torchsummary import summary\n",
        "# summary(net_090, input_size=(3, 128, 128))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JYCbqA4cfTb",
        "colab_type": "text"
      },
      "source": [
        "## 畫出圖片"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_m_Yz9Yfaqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters_1 = []\n",
        "acc_1 = []\n",
        "for i in range(len(parameters_acc)):\n",
        "  parameters_1.append(parameters_acc[i][0])\n",
        "  acc_1.append(parameters_acc[i][1])\n",
        "\n",
        "parameters_2 = []\n",
        "acc_2 = []\n",
        "for i in range(len(parameters_acc_090)):\n",
        "  parameters_2.append(parameters_acc_090[i][0])\n",
        "  acc_2.append(parameters_acc_090[i][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhJ3-bKUciYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Comparison between the different pruning rate')\n",
        "plt.plot(parameters_2, acc_2, label='pruning 0.9') #pruning 0.9\n",
        "plt.plot(parameters_1, acc_1, label='pruning 0.95') #pruning 0.95\n",
        "\n",
        "plt.xlabel('num of parameters')\n",
        "plt.ylabel('val_acc')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFu_opsbl3sS",
        "colab_type": "text"
      },
      "source": [
        "# Inference\n",
        "\n",
        "同Hw3，請參考該作業:)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6484CAKLeJkH",
        "colab_type": "text"
      },
      "source": [
        "## testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2wmtDPGeJGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataloader = get_dataloader('testing', batch_size=32)\n",
        "print('finish test_dataloader')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBlb9m8eeNbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "net.eval()\n",
        "prediction = []\n",
        "\n",
        "for now_step, batch_data in enumerate(test_dataloader):\n",
        "    # 清空 optimizer\n",
        "    optimizer.zero_grad()\n",
        "    # 處理 input\n",
        "    inputs, labels = batch_data\n",
        "    inputs = inputs.cuda()\n",
        "    # labels = labels.cuda() \n",
        "    with torch.no_grad():\n",
        "        logits = net(inputs)\n",
        "        test_label = np.argmax(logits.cpu().data.numpy(), axis=1)\n",
        "        for y in test_label:\n",
        "            prediction.append(y)     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Teb1ffUeOpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 丟到 hw7\n",
        "from google.colab import files\n",
        "\n",
        "#將結果寫入 csv 檔\n",
        "with open(\"predict.csv\", 'w') as f:\n",
        "    f.write('Id,label\\n')\n",
        "    for i, y in  enumerate(prediction):\n",
        "        f.write('{},{}\\n'.format(i, y))\n",
        "#存到本機端\n",
        "files.download('predict.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv9Os3CLIq2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 丟到 hw3\n",
        "from google.colab import files\n",
        "\n",
        "#將結果寫入 csv 檔\n",
        "with open(\"predict.csv\", 'w') as f:\n",
        "    f.write('Id,Category\\n')\n",
        "    for i, y in  enumerate(prediction):\n",
        "        f.write('{},{}\\n'.format(i, y))\n",
        "#存到本機端\n",
        "files.download('predict.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIcblvbUCTOP",
        "colab_type": "text"
      },
      "source": [
        "# Q&A\n",
        "\n",
        "有任何問題Network Compression的問題可以寄信到b05902127@ntu.edu.tw。\n",
        "\n",
        "時間允許的話我會更新在這裡。"
      ]
    }
  ]
}